# @package _global_

# to execute this experiment run:
# python train.py experiment=jetclass

defaults:
  - override /data: classifier_data_jetclass.yaml
  - override /model: particlenet_classifier.yaml
  # - override /model: ParT_classifier.yaml
  - override /trainer: ddp.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# add here checkpoint to continue training
# ckpt_path: /beegfs/desy/user/birkjosc/epic-fm/logs/jetclass_cond_jettype/runs/2023-08-10_16-36-26/checkpoints/last-EMA.ckpt

tags: ["fm-classifier_test", "JetClass", "ClassifierTest"]

run_note: ""

seed: 160397

data:
  kin_only: true

trainer:
  min_epochs: 1
  max_epochs: 100
  gradient_clip_val: 0.5

# setting load_weights_from will load the weights from the given checkpoint, but start training from scratch
# load_weights_from: /beegfs/desy/user/birkjosc/epic-fm/logs/jetclass/runs/2023-08-24_15-36-42/checkpoints/epoch_160_loss_40.79013-EMA.ckpt

callbacks: null

model:
  input_dims: 7  # ParticleNet
  # input_dim: 7  # ParT
  num_classes: 2
#   num_particles: 128
#   global_cond_dim: 13 # needs to be calculated when using conditioning (= number of jet types + additional conditioning variables)
#   local_cond_dim: 0
#   features: 4  # = 3 + number of `additional_part_features` (see below)
#   layers: 13 # default is 6
#   hidden_dim: 250 # default is 128
#   latent: 15 # default is 10
#   scheduler:
#     warmup: 50
#     max_iters: 1000

  #early_stopping:
  #  monitor: "val/loss"
  #  patience: 2000
  #  mode: "min"

task_name: "jetclass"

logger:
  wandb:
    tags: ${tags}
    group: "flow_matching_jetclass"
    name: ${task_name}
  comet:
    experiment_name: null
    project_name: "flow-matching-classifierTest"

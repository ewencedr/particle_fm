# @package _global_

# to execute this experiment run:
# python train.py experiment=jetclass_classifier

defaults:
  - override /data: classifier_data_jetclass.yaml
  # - override /model: mlp_classifier.yaml
  # - override /model: particlenet_classifier.yaml
  # - override /model: particlenet_lite_classifier.yaml
  - override /model: ParT_classifier.yaml
  - override /callbacks: jetclass_classifier.yaml
  - override /trainer: gpu.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# add here checkpoint to continue training
# ckpt_path: /beegfs/desy/user/birkjosc/epic-fm/logs/jetclass_cond_jettype/runs/2023-08-10_16-36-26/checkpoints/last-EMA.ckpt

tags: ["fm-classifier_test", "JetClass", "ClassifierTest"]
run_note: ""
seed: 12

vars:
  epochs: 50
  warmup: 3
  val_check_interval: null
  used_flavor: Hbb

data:
  batch_size: 256  # ParT
  # batch_size: 1024  # ParticleNet-Lite
  # train_val_test_split: [0.8, 0.1, 0.1]
  # kin_only: true
  set_neutral_particle_ips_zero: true
  set_energy_equal_to_p: true
  pf_features_list:
    - part_etarel
    - part_dphi
    - log_part_pt
    - log_part_energy
    - log_part_ptrel
    - log_part_energyrel
    - part_deltaR
    # - part_charge
    # - part_isChargedHadron
    # - part_isNeutralHadron
    # - part_isPhoton
    # - part_isElectron
    # - part_isMuon
    # - tanh_part_d0val
    # - part_d0err
    # - tanh_part_dzval
    # - part_dzerr
  # data_file: /beegfs/desy/user/birkjosc/epic-fm/logs/jetclass/runs/2023-09-11_14-22-33/evaluated_ckpts/epoch_753/generated_data_epoch_753_nsamples_1000000.h5
  # data_file: /beegfs/desy/user/birkjosc/epic-fm/logs/jetclass/runs/2023-09-11_14-22-33/evaluated_ckpts/epoch_753/generated_data_epoch_753_nsamples_100000-truth_cond.h5
  # data_file: /beegfs/desy/user/birkjosc/epic-fm/logs/jetclass/runs/2023-09-11_14-22-33/evaluated_ckpts/epoch_753/generated_data_epoch_753_nsamples_100000.h5
  # data_file: /beegfs/desy/user/birkjosc/epic-fm/logs/jetclass/runs/2023-09-16_19-12-11/evaluated_ckpts/epoch_3744/generated_data_epoch_3744_nsamples_100000.h5
  # landscape dataset for crosscheck
  # data_file: /beegfs/desy/user/birkjosc/datasets/landscape/TopLandscape/trainvaltest_file_for_crosscheck_classifier_simulated_generated.h5
  # data_file: /beegfs/desy/user/birkjosc/epic-fm/logs/jetclass/runs/2023-10-03_08-48-00/evaluated_ckpts/epoch_499/eval_output_epoch_499_nsamples_1000000.h5
  # data_file: /beegfs/desy/user/birkjosc/epic-fm/logs/jetclass/runs/2023-10-03_08-48-00/evaluated_ckpts/epoch_499/eval_output_epoch_499_nsamples_500000-top_jets_only.h5
  # data_file: /beegfs/desy/user/birkjosc/epic-fm/logs/jetclass/runs/2023-10-03_08-48-00/evaluated_ckpts/epoch_499/eval_output_epoch_499_nsamples_500000-top_jets_only_new_fix.h5
  # data_file: /beegfs/desy/user/birkjosc/epic-fm/logs/jetclass/runs/2023-10-03_08-48-00/evaluated_ckpts/epoch_499/eval_output_epoch_499_nsamples_500000-Hbb_only.h5
    # "QCD", "Hbb", "Hcc", "Hgg", "H4q", "Hqql", "Zqq", "Wqq", "Tbqq", "Tbl",
  used_flavor: ${vars.used_flavor}
  data_file: /beegfs/desy/user/birkjosc/epic-fm/logs/jetclass/runs/2023-10-03_08-48-00/evaluated_ckpts/epoch_499/eval_output_epoch_499_nsamples_500000-${vars.used_flavor}_only_new_conditioning.h5
  number_of_jets: 500000
  train_val_test_split: [0.5, 0.1, 0.4]
  # debug_sim_only: true
  # debug_sim_gen_fraction: 0.8

# setting load_weights_from will load the weights from the given checkpoint, but start training from scratch
# load_weights_from: /beegfs/desy/user/birkjosc/epic-fm/logs/jetclass/runs/2023-08-24_15-36-42/checkpoints/epoch_160_loss_40.79013-EMA.ckpt

model:
  input_dim: 7  # this is renamed to "input_dims" for ParticleNet
  # conv_params:
  #     - [7, [16, 16, 16]]
  #     - [7, [16, 16, 16]]
  # fc_params: [[64, 0.1]] # ParticleNet
  num_classes: 2
  load_pretrained: true
  # no scheduler overwrite for ParT
  # scheduler:
  #   # ParticleNet-Lite scheduler
  #   max_iters: ${vars.epochs}
  #   warmup: 50
  #   cooldown: 50
  #   cooldown_final: 50
  #   max_lr: 0.005
  #   initial_lr: 0.0005
  #   final_lr: 0.000001
  #   # max_iters: 20
  # optimizer:
  #   weight_decay: 0.0001
    # lr: 0.001

  # early_stopping:
  #  monitor: "val_loss"
  #  patience: 20
  #  mode: "min"

task_name: "jetclass_classifier"

trainer:
  min_epochs: 1
  max_epochs: ${vars.epochs}
  val_check_interval: ${vars.val_check_interval}
  gradient_clip_val: 0.5  # ParticleNet 0.02, ParticleNet-Lite 0.1
  gradient_clip_algorithm: norm

logger:
  wandb:
    tags: ${tags}
    group: "flow_matching_jetclass"
    name: ${task_name}
  comet:
    experiment_name: null
    project_name: "flow-matching-classifierTest"

# @package _global_

# to execute this experiment run:
# python train.py experiment=jetclass_classifier

defaults:
  - override /data: classifier_data_jetclass.yaml
  # - override /model: mlp_classifier.yaml
  - override /model: particlenet_classifier.yaml
  # - override /model: particlenet_lite_classifier.yaml
  # - override /model: ParT_classifier.yaml
  - override /callbacks: jetclass_classifier.yaml
  - override /trainer: gpu.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# add here checkpoint to continue training
# ckpt_path: /beegfs/desy/user/birkjosc/epic-fm/logs/jetclass_cond_jettype/runs/2023-08-10_16-36-26/checkpoints/last-EMA.ckpt

tags: ["fm-classifier_test", "JetClass", "ClassifierTest"]
run_note: ""
seed: 122

vars:
  epochs: 100
  warmup: 3
  val_check_interval: null

data:
  batch_size: 1024
  train_val_test_split: [0.8, 0.1, 0.1]
  # kin_only: true
  pf_features_list:
    - part_etarel
    - part_dphi
    - log_part_pt
    # - log_part_energy
    # - log_part_ptrel
    # - log_part_energyrel
    - part_deltaR
    # - part_charge
    # - part_isChargedHadron
    # - part_isNeutralHadron
    # - part_isPhoton
    # - part_isElectron
    # - part_isMuon
    # - tanh_part_d0val
    # - part_d0err
    # - tanh_part_dzval
    # - part_dzerr
  data_file: /beegfs/desy/user/birkjosc/epic-fm/logs/jetclass/runs/2023-09-11_14-22-33/evaluated_ckpts/epoch_753/generated_data_epoch_753_nsamples_1000000.h5
  # data_file: /beegfs/desy/user/birkjosc/epic-fm/logs/jetclass/runs/2023-09-11_14-22-33/evaluated_ckpts/epoch_753/generated_data_epoch_753_nsamples_100000-truth_cond.h5
  # data_file: /beegfs/desy/user/birkjosc/epic-fm/logs/jetclass/runs/2023-09-11_14-22-33/evaluated_ckpts/epoch_753/generated_data_epoch_753_nsamples_100000.h5
  # data_file: /beegfs/desy/user/birkjosc/epic-fm/logs/jetclass/runs/2023-09-16_19-12-11/evaluated_ckpts/epoch_3744/generated_data_epoch_3744_nsamples_100000.h5
  # number_of_jets: 1000
  # debug_sim_only: true
  # debug_sim_gen_fraction: 0.8
  # used_flavor: QCD

# setting load_weights_from will load the weights from the given checkpoint, but start training from scratch
# load_weights_from: /beegfs/desy/user/birkjosc/epic-fm/logs/jetclass/runs/2023-08-24_15-36-42/checkpoints/epoch_160_loss_40.79013-EMA.ckpt

model:
  input_dims: 4  # ParticleNet
  input_dim: 7  # ParT
  # conv_params:
  #     - [7, [16, 16, 16]]
  #     - [7, [16, 16, 16]]
  # fc_params: [[64, 0.1]] # ParticleNet
  num_classes: 2
  # load_pretrained: true
  scheduler:
  #   warmup: ${vars.warmup}
    max_iters: ${vars.epochs}
  # scheduler:
    # warmup: 3
    # cooldown: 4
    # cooldown_final: 5
    # max_lr: 0.005
    # initial_lr: 0.0005
    # final_lr: 0.000001
    # max_iters: 20
  optimizer:
    weight_decay: 0.01
    # lr: 0.001

  # early_stopping:
  #  monitor: "val_loss"
  #  patience: 20
  #  mode: "min"

task_name: "jetclass_classifier"

trainer:
  min_epochs: 1
  max_epochs: ${vars.epochs}
  val_check_interval: ${vars.val_check_interval}
  gradient_clip_val: null

logger:
  wandb:
    tags: ${tags}
    group: "flow_matching_jetclass"
    name: ${task_name}
  comet:
    experiment_name: null
    project_name: "flow-matching-classifierTest"

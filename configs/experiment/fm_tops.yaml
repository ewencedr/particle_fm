# @package _global_

# to execute this experiment run:
# python train.py experiment=fm_tops

defaults:
  - override /data: /jetnet/jetnet_tops_30.yaml
  - override /model: cnf_flow_matching.yaml
  - override /callbacks: default.yaml
  - override /trainer: gpu.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters



tags: ["flow_matching", "tops"]

trainer:
  min_epochs: 10
  max_epochs: 100
  gradient_clip_val: 0.5

model:
  optimizer:
    lr: 0.002
  features: 3
  num_particles: 30
  embedding_dim: 128
  heads: 4
  depth: 6
  #scheduler:
  #  warmup: ${trainer.min_epochs}
  #  max_iters: ${trainer.max_epochs}

data:
  batch_size: 256
  variable_jet_sizes: False
  centering: True
  normalize: False
  use_calculated_base_distribution: False

callbacks:
  model_checkpoint:
    dirpath: ${paths.output_dir}/checkpoints
    filename: "epoch_{epoch:03d}_loss_{val/loss:.3f}"
    monitor: "val/loss"
    mode: "max"
    save_last: True
    auto_insert_metric_name: False

  early_stopping:
    monitor: "val/loss"
    patience: 6
    mode: "max"

  #jetnet_eval:
  #  logger: 1
  #  every_n_epochs: 10
  #  num_jet_samples: 5000
  #  w_dists_batches: 2
  #  sampling_batch_size: 1000
  #  test_particle_data: None
  #  test_mask: None
  #  means: None
  #  stds: None

logger:
  wandb:
    tags: ${tags}
    group: "fm_tops"


task_name: "fm_tops-${model.num_particles}"

# @package _global_

# to execute this experiment run:
# python train.py experiment=fm_tops

defaults:
  - override /data: /jetnet/jetnet_tops_30.yaml
  - override /model: cnf_flow_matching.yaml
  - override /callbacks: jetnet.yaml
  - override /trainer: gpu.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters



tags: ["flow_matching", "tops", "30", "test_runs"]

trainer:
  min_epochs: 3
  max_epochs: 30
  gradient_clip_val: 0.5

model:
  optimizer:
    lr: 0.002
  model: "epic"
  features: 3
  hidden_dim: 128
  num_particles: 30
  frequencies: 6
  use_mass_loss: False
  layers: 8
  n_transforms: 1
  activation: leaky_relu
  wrapper_func: weight_norm
  # epic
  latent: 16
  return_latent_space: False
  mass_conditioning: True
  global_cond_dim: 10
  local_cond_dim: 1
  t_local_cat: True
  t_global_cat: False
  # transformer
  dropout: 0.0
  heads: 4
  mask: False
  scheduler:
    warmup: ${trainer.min_epochs}
    max_iters: ${trainer.max_epochs}
  #scheduler:
#  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
#  _partial_: true
#  T_max: 100

data:
  batch_size: 256
  variable_jet_sizes: False
  centering: True
  normalize: False
  use_calculated_base_distribution: False

callbacks:
  model_checkpoint:
    dirpath: ${paths.output_dir}/checkpoints
    filename: "epoch_{epoch:03d}_loss_{val/loss:.5f}"
    monitor: "val/loss"
    mode: "min"
    save_last: True
    auto_insert_metric_name: False

  early_stopping:
    monitor: "val/loss"
    patience: 2000
    mode: "min"

  jetnet_eval:
    logger: 0
    every_n_epochs: 3
    num_jet_samples: 3000
    batch_size: 1000
    plot_w_dists: False
    w_dists_batches: 3
    mass_conditioning: ${model.mass_conditioning}


task_name: "fm_tops-${model.num_particles}"

logger:
  wandb:
    project: "test_runs"
    tags: ${tags}
    group: "fm_tops"
    name: ${task_name}
  comet:
    experiment_name: ${task_name}
    project_name: "test_runs"

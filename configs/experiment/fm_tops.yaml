# @package _global_

# to execute this experiment run:
# python train.py experiment=fm_tops

defaults:
  - override /data: /jetnet/jetnet_tops_30.yaml
  - override /model: cnf_flow_matching.yaml
  - override /callbacks: jetnet.yaml
  - override /trainer: gpu.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters



tags: ["flow_matching", "tops", "30", "test_runs"]

#seed: 12345

trainer:
  min_epochs: 3
  max_epochs: 30
  gradient_clip_val: 0.5
  detect_anomaly: true # raise exception if NaN or +/-inf is detected in any tensor

model:
  optimizer:
    lr: 0.001
  optimizer_d:
    lr: 0.0001
  model: "epic"
  features: 3
  hidden_dim: 128
  num_particles: 150
  frequencies: 12
  use_mass_loss: False
  layers: 6
  n_transforms: 1
  activation: leaky_relu
  wrapper_func: weight_norm
  # epic
  latent: 16
  return_latent_space: False
  mass_conditioning: False
  global_cond_dim: 0
  local_cond_dim: 0
  t_local_cat: True
  t_global_cat: False

  dropout: 0.0

  # transformer
  heads: 4
  mask: True
  # debug
  plot_loss_hist_debug: True
  # loss
  loss_type: FM-OT
  sigma: 1e-4
  loss_comparison: MSE
  loss_type_d: LSGAN
  t_emb: gaussian
  # scheduler
  scheduler:
    warmup: ${trainer.min_epochs}
    max_iters: ${trainer.max_epochs}
  #cosine_annealing:
  #scheduler:
  #  T_max: ${trainer.max_epochs}

data:
  jet_type:
    #- "g"
    #- "q"
    - "t"
    #- "w"
    #- "z"
  batch_size: 256
  num_particles: ${model.num_particles}
  variable_jet_sizes: ${model.mask}
  centering: True
  normalize: True
  normalize_sigma: 5
  use_calculated_base_distribution: False
  conditioning_type: False
  conditioning_pt: False
  conditioning_eta: False
  conditioning_mass: False
  conditioning_num_particles: False

callbacks:

  ema:
    decay: 0.9999
    apply_ema_every_n_steps: 1
    start_step: 0
    save_ema_weights_in_callback_state: True
    evaluate_ema_weights_instead: True


  jetnet_eval:
    every_n_epochs: 3
    num_jet_samples: 2
    batch_size: 1024
    plot_w_dists: False
    w_dists_batches: 2
    log_w_dists: True
    mass_conditioning: ${model.mass_conditioning}
    data_type: "test"
    use_ema: ${callbacks.ema.evaluate_ema_weights_instead}
    ode_solver: "midpoint"

  model_checkpoint:
    dirpath: ${paths.output_dir}/checkpoints
    filename: "epoch_{epoch:03d}_loss_{val/loss:.5f}"
    monitor: "val/loss"
    mode: "min"
    save_last: True
    auto_insert_metric_name: False
    save_top_k: 2

  model_checkpoint2:
    dirpath: ${paths.output_dir}/checkpoints
    filename: "epoch_{epoch:03d}_w1m_{w1m_mean_1b:.8f}"
    monitor: "w1m_mean_1b"
    mode: "min"
    auto_insert_metric_name: False
    every_n_epochs: 1 # ${callbacks.jetnet_eval.every_n_epochs}+1
    save_top_k: 2

  model_checkpoint3:
    dirpath: ${paths.output_dir}/checkpoints
    filename: "epoch_{epoch:03d}_w1p_{w1p_mean_1b:.8f}"
    monitor: "w1p_mean_1b"
    mode: "min"
    auto_insert_metric_name: False
    every_n_epochs: 1
    save_top_k: 2

  #early_stopping:
  #  monitor: "val/loss"
  #  patience: 2000
  #  mode: "min"

task_name: "fm_tops-${model.num_particles}-1"

logger:
  wandb:
    project: "test_runs"
    tags: ${tags}
    group: "fm_tops"
    name: ${task_name}
  comet:
    experiment_name: ${task_name}
    project_name: "test_runs"

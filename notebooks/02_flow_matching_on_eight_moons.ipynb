{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Flow Matching on EightMoons\n",
    "\n",
    "This notebook is a simple example of how to use different Flow Matching loss objective. The following Flow Matching models are implemented:\n",
    "    \n",
    "* Conditional Flow Matching from [2302.00482](https://arxiv.org/abs/2302.00482)\n",
    "    \n",
    "* Optimal Transport CFM from [2302.00482](https://arxiv.org/abs/2302.00482)\n",
    "\n",
    "* Self conditioned Flow Matching as in [2310.05764](https://arxiv.org/abs/2310.05764)\n",
    "\n",
    "As this repository mainly focuses on the generation of point clouds, the examples also consider point cloud structured data. Note, that this data structure complicates the problem and the trainings might take a few minutes to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ot as pot\n",
    "import torch\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# import torchdyn\n",
    "from torchdyn.core import NeuralODE\n",
    "from torchdyn.datasets import generate_moons\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "savedir = \"models/8gaussian-moons\"\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from particle_fm.models.components.epic import EPiC_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement some helper functions\n",
    "\n",
    "\n",
    "def eight_normal_sample(n, dim, scale=1, var=1):\n",
    "    m = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "        torch.zeros(dim), math.sqrt(var) * torch.eye(dim)\n",
    "    )\n",
    "    centers = [\n",
    "        (1, 0),\n",
    "        (-1, 0),\n",
    "        (0, 1),\n",
    "        (0, -1),\n",
    "        (1.0 / np.sqrt(2), 1.0 / np.sqrt(2)),\n",
    "        (1.0 / np.sqrt(2), -1.0 / np.sqrt(2)),\n",
    "        (-1.0 / np.sqrt(2), 1.0 / np.sqrt(2)),\n",
    "        (-1.0 / np.sqrt(2), -1.0 / np.sqrt(2)),\n",
    "    ]\n",
    "    centers = torch.tensor(centers) * scale\n",
    "    noise = m.sample((n,))\n",
    "    multi = torch.multinomial(torch.ones(8), n, replacement=True)\n",
    "    data = []\n",
    "    for i in range(n):\n",
    "        data.append(centers[multi[i]] + noise[i])\n",
    "    data = torch.stack(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def sample_moons(n, num_points=30):\n",
    "    # x0, _ = generate_moons(n, noise=0.2)\n",
    "    # return x0 * 3 - 1\n",
    "    data = []\n",
    "    for _ in range(n):\n",
    "        x, _ = make_moons(n_samples=num_points, noise=0.05, shuffle=True)\n",
    "        data.append(x)\n",
    "    return torch.tensor(np.array(data), dtype=torch.float32) * 3 - 1\n",
    "\n",
    "\n",
    "def sample_8gaussians(n, num_points=30):\n",
    "    data = []\n",
    "    for _ in range(n):\n",
    "        x = eight_normal_sample(num_points, 2, scale=5, var=0.1).float()\n",
    "        data.append(np.array(x))\n",
    "    return torch.tensor(np.array(data), dtype=torch.float32)\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, dim, out_dim=None, w=64, time_varying=False):\n",
    "        super().__init__()\n",
    "        self.time_varying = time_varying\n",
    "        if out_dim is None:\n",
    "            out_dim = dim\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(dim + (1 if time_varying else 0), w),\n",
    "            torch.nn.SELU(),\n",
    "            torch.nn.Linear(w, w),\n",
    "            torch.nn.SELU(),\n",
    "            torch.nn.Linear(w, w),\n",
    "            torch.nn.SELU(),\n",
    "            torch.nn.Linear(w, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class GradModel(torch.nn.Module):\n",
    "    def __init__(self, action):\n",
    "        super().__init__()\n",
    "        self.action = action\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.requires_grad_(True)\n",
    "        grad = torch.autograd.grad(torch.sum(self.action(x)), x, create_graph=True)[0]\n",
    "        return grad[:, :-1]\n",
    "\n",
    "\n",
    "class torch_wrapper(torch.nn.Module):\n",
    "    \"\"\"Wraps model to torchdyn compatible format.\"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        return self.model(torch.cat([x, t.repeat(x.shape[:-1])[..., None]], dim=-1))\n",
    "\n",
    "\n",
    "class torch_wrapper_epic(torch.nn.Module):\n",
    "    \"\"\"Wraps model to torchdyn compatible format.\"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, t, x, *args, **kwargs):\n",
    "        x_local = torch.cat([x, t.repeat(x.shape[:-1])[..., None]], dim=-1)\n",
    "        x_global = torch.randn_like(torch.ones(x_local.shape[0], 16, device=x_local.device))\n",
    "        return self.model(t, x_local)\n",
    "\n",
    "        # \"#B6BFC3\",\n",
    "        # \"#3B515B\",\n",
    "        # \"#0271BB\",\n",
    "        # \"#E2001A\",\n",
    "\n",
    "\n",
    "def plot_trajectories(traj, n=20, save_name=\"test\"):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.xlim(-7, 7)\n",
    "    plt.ylim(-7, 8)\n",
    "    plt.scatter(\n",
    "        traj[0, :n, :, 0],\n",
    "        traj[0, :n, :, 1],\n",
    "        s=10,\n",
    "        alpha=0.8,\n",
    "        c=\"#0271BB\",\n",
    "        label=\"Prior samples $x_1$\",\n",
    "    )\n",
    "    plt.scatter(\n",
    "        traj[:, :n, :, 0], traj[:, :n, :, 1], s=0.2, alpha=0.1, c=\"#3B515B\", label=\"Flow $x_t$\"\n",
    "    )\n",
    "    plt.scatter(\n",
    "        traj[-1, :n, :, 0],\n",
    "        traj[-1, :n, :, 1],\n",
    "        s=4,\n",
    "        alpha=1,\n",
    "        c=\"#E2001A\",\n",
    "        label=\"Gen. samples $x_0$\",\n",
    "    )\n",
    "    legend = plt.legend(frameon=False, loc=\"upper left\")\n",
    "    # plt.xticks([])\n",
    "    # plt.yticks([])\n",
    "    legend.legend_handles[0]._sizes = [30]\n",
    "    legend.legend_handles[1]._sizes = [30]\n",
    "    legend.legend_handles[2]._sizes = [30]\n",
    "\n",
    "    plt.savefig(f\"{save_name}.pdf\", bbox_inches=\"tight\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "moons = sample_moons(100)\n",
    "print(moons.shape)\n",
    "gaussians = sample_8gaussians(100)\n",
    "print(gaussians.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(moons[:10, :, 0], moons[:10, :, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(gaussians[:10, :, 0], gaussians[:10, :, 1])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Conditional Flow Matching\n",
    "The Conditional Flow Matching Objective allows the usage of non Gaussian priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "steps = 200\n",
    "sigma = 0.1\n",
    "dim = 2\n",
    "batch_size = 256\n",
    "# model = MLP(dim=dim, time_varying=True)\n",
    "model = EPiC_encoder(feats=dim, input_dim=dim + 1)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "start = time.time()\n",
    "for k in tqdm(range(steps)):\n",
    "    optimizer.zero_grad()\n",
    "    x0 = sample_8gaussians(batch_size)\n",
    "    x1 = sample_moons(batch_size)\n",
    "\n",
    "    t = torch.rand_like(torch.ones(x0.shape[0]))\n",
    "    t = t.unsqueeze(-1).repeat_interleave(x0.shape[1], dim=1).unsqueeze(-1)\n",
    "    t = t.type_as(x0)\n",
    "\n",
    "    mu_t = (1 - t) * x1 + t * x0\n",
    "\n",
    "    sigma_t = sigma\n",
    "    x = mu_t + sigma_t * torch.randn_like(x0)\n",
    "\n",
    "    ut = x0 - x1\n",
    "\n",
    "    x_local = torch.cat([x, (t)], dim=-1)\n",
    "    x_global = torch.randn_like(torch.ones(x_local.shape[0], 16, device=x_local.device))\n",
    "\n",
    "    vt = model(t, x_local)\n",
    "\n",
    "    loss = torch.mean((vt - ut) ** 2)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (k + 1) % 50 == 0:\n",
    "        end = time.time()\n",
    "        print(f\"{k+1}: loss {loss.item():0.3f} time {(end - start):0.2f}\")\n",
    "        start = end\n",
    "        node = NeuralODE(\n",
    "            torch_wrapper_epic(model),\n",
    "            solver=\"midpoint\",\n",
    "            sensitivity=\"adjoint\",\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            traj = node.trajectory(\n",
    "                sample_8gaussians(100),\n",
    "                t_span=torch.linspace(1.0, 0.0, 50),\n",
    "            )\n",
    "            plot_trajectories(traj)\n",
    "# torch.save(model, f\"{savedir}/cfm_v1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this shows how easy one can implement an euler solver\n",
    "\n",
    "\n",
    "def esampler(func, noise, steps=50):\n",
    "    t = torch.ones(1)\n",
    "    delta_t = 1 / steps\n",
    "    for _ in range(steps):\n",
    "        noise += -func(t, noise) * delta_t\n",
    "        t -= delta_t\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    zss = sample_8gaussians(100)\n",
    "    xss = esampler(torch_wrapper_epic(model), zss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(xss[:, :, 0], xss[:, :, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trajectories(traj, n=20, save_name=\"cfm\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Optimal Transport Conditional Flow Matching\n",
    "\n",
    "In this flow matching implementation, an the marginal probability paths are ensured to be optimal. The original paper did this in mini-batches, in this implementation for sets, it is done for the whole set. This implementation creates very nice paths but the calculation of the optimal transport map makes it really slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sigma = 0.1\n",
    "dim = 2\n",
    "batch_size = 256\n",
    "# model = MLP(dim=dim, time_varying=True)\n",
    "model = EPiC_encoder(feats=dim)  # latent_local=dim + 1)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "start = time.time()\n",
    "for z in tqdm(range(2000)):\n",
    "    optimizer.zero_grad()\n",
    "    x0 = sample_8gaussians(batch_size)  # (x,y)\n",
    "    x1 = sample_moons(batch_size)  # (x,y)\n",
    "\n",
    "    t = torch.rand_like(torch.ones(x0.shape[0]))\n",
    "    t = t.unsqueeze(-1).repeat_interleave(x0.shape[1], dim=1).unsqueeze(-1)\n",
    "    t = t.type_as(x0)\n",
    "\n",
    "    a, b = pot.unif(x0.size()[1]), pot.unif(x1.size()[1])\n",
    "    a = np.repeat(np.expand_dims(a, axis=0), x0.size()[0], axis=0)\n",
    "    b = np.repeat(np.expand_dims(b, axis=0), x1.size()[0], axis=0)\n",
    "    M = torch.cdist(x0, x1) ** 2\n",
    "    for k in range(M.shape[0]):\n",
    "        M[k] = M[k] / M[k].max()\n",
    "        pi = pot.emd(a[k], b[k], M[k].detach().cpu().numpy())\n",
    "        p = pi.flatten()\n",
    "        p = p / p.sum()\n",
    "        choices = np.random.choice(pi.shape[0] * pi.shape[1], p=p, size=pi.shape[0])\n",
    "        i, j = np.divmod(choices, pi.shape[1])\n",
    "        x0[k] = x0[k, i]\n",
    "        x1[k] = x1[k, j]\n",
    "\n",
    "    mu_t = x0 * t + x1 * (1 - t)\n",
    "    sigma_t = sigma\n",
    "    x = mu_t + sigma_t * torch.randn_like(x0)\n",
    "    ut = x0 - x1\n",
    "    x_local = torch.cat([x, t], dim=-1)\n",
    "    x_global = torch.randn_like(torch.ones(x_local.shape[0], 16, device=x_local.device))\n",
    "    vt = model(t, x_local)\n",
    "    loss = torch.mean((vt - ut) ** 2)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (z + 1) % 500 == 0:\n",
    "        end = time.time()\n",
    "        print(f\"{z+1}: loss {loss.item():0.3f} time {(end - start):0.2f}\")\n",
    "        start = end\n",
    "        node0 = NeuralODE(\n",
    "            torch_wrapper_epic(model),\n",
    "            solver=\"midpoint\",\n",
    "            sensitivity=\"adjoint\",\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            traj0 = node0.trajectory(\n",
    "                sample_8gaussians(100),\n",
    "                t_span=torch.linspace(1, 0, 50),\n",
    "            )\n",
    "            plot_trajectories(traj0)\n",
    "# torch.save(model, f\"{savedir}/cfm_v1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trajectories(traj, n=20, save_name=\"cfm_ot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# Self Conditioned Model\n",
    "The CFM loss is combined with an EPiC architecture and self-conditioning. This requires also a special solver and not every standard solver can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified euler solver\n",
    "\n",
    "\n",
    "def esampler_selfcond(func, noise, steps=100):\n",
    "    traj_arr = []\n",
    "    t = torch.ones(1)\n",
    "    delta_t = 1 / steps\n",
    "    for _ in range(steps):\n",
    "        if t == 1:\n",
    "            cond = -func(t, noise, None)\n",
    "            noise += cond * delta_t\n",
    "        else:\n",
    "            cond = -func(t, noise, cond)\n",
    "            noise += cond * delta_t\n",
    "        t -= delta_t\n",
    "        traj_arr.append(noise.clone().numpy())\n",
    "    return noise, np.array(traj_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class torch_wrapper_epic_selfcond(torch.nn.Module):\n",
    "    \"\"\"Wraps model to solver compatible format.\"\"\"\n",
    "\n",
    "    def __init__(self, model, cond):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.cond = cond\n",
    "\n",
    "    def forward(self, t, x, cond, *args, **kwargs):\n",
    "        x_local = torch.cat([x, t.repeat(x.shape[:-1])[..., None]], dim=-1)\n",
    "        x_global = torch.randn_like(torch.ones(x_local.shape[0], 16, device=x_local.device))\n",
    "        if cond is None:\n",
    "            x_local = torch.cat([x_local, self.cond], dim=-1)\n",
    "        else:\n",
    "            x_local = torch.cat([x_local, cond], dim=-1)\n",
    "        return self.model(t, x_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "steps = 1000\n",
    "sigma = 0.1\n",
    "dim = 2\n",
    "batch_size = 256\n",
    "# model = MLP(dim=dim, time_varying=True)\n",
    "model = EPiC_encoder(feats=dim, input_dim=dim + 1 + 2)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "start = time.time()\n",
    "for k in tqdm(range(steps)):\n",
    "    optimizer.zero_grad()\n",
    "    x0 = sample_8gaussians(batch_size)\n",
    "    x1 = sample_moons(batch_size)\n",
    "\n",
    "    t = torch.rand_like(torch.ones(x0.shape[0]))\n",
    "    t = t.unsqueeze(-1).repeat_interleave(x0.shape[1], dim=1).unsqueeze(-1)\n",
    "    t = t.type_as(x0)\n",
    "\n",
    "    s = torch.rand(1)\n",
    "\n",
    "    mu_t = (1 - t) * x1 + t * x0\n",
    "\n",
    "    sigma_t = sigma\n",
    "    x = mu_t + sigma_t * torch.randn_like(x0)\n",
    "\n",
    "    ut = x0 - x1\n",
    "\n",
    "    x_local = torch.cat([x, (t)], dim=-1)\n",
    "\n",
    "    x_1_tilde = sample_8gaussians(batch_size)\n",
    "    if s > 0.5:\n",
    "        x_local2 = torch.cat([x_local, x_1_tilde], dim=-1)\n",
    "        x_1_tilde = model(t, x_local2, x_1_tilde)\n",
    "\n",
    "    x_global = torch.randn_like(torch.ones(x_local.shape[0], 16, device=x_local.device))\n",
    "\n",
    "    x_local = torch.cat([x_local, x_1_tilde], dim=-1)\n",
    "    vt = model(t, x_local)\n",
    "\n",
    "    loss = torch.mean((vt - ut) ** 2)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (k + 1) % 50 == 0 or k == 0:\n",
    "        end = time.time()\n",
    "        print(f\"{k+1}: loss {loss.item():0.3f} time {(end - start):0.2f}\")\n",
    "        start = end\n",
    "        with torch.no_grad():\n",
    "            torch.manual_seed(0)\n",
    "            zss = sample_8gaussians(20)\n",
    "            xss, traj = esampler_selfcond(torch_wrapper_epic_selfcond(model, zss), zss)\n",
    "        plot_trajectories(traj, n=20, save_name=f\"pictures/{k+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    zss = sample_8gaussians(100)\n",
    "    xss, traj = esampler_selfcond(torch_wrapper_epic_selfcond(model, zss), zss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trajectories(traj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pllhome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

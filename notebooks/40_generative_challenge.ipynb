{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from src.data.components import normalize_tensor\n",
    "from src.utils.preprocessing import LogitScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set env variable DATA_DIR again because of hydra\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"DATA_DIR\"] = os.environ.get(\"DATA_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/beegfs/desy/user/sommerhm/generative_challenge_2023/outerdata_kfold_1.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data[:, 0], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 5):\n",
    "    plt.hist(data[:, i], bins=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.mean(data, axis=0)\n",
    "stds = np.std(data, axis=0)\n",
    "normalized_data_1 = normalize_tensor(np.copy(data), means, stds, sigma=1)\n",
    "print(means)\n",
    "print(stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(data)\n",
    "print(scaler.mean_)\n",
    "print(scaler.scale_)\n",
    "normalized_data_2 = scaler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for i in range(5):\n",
    "    hist = ax[i].hist(normalized_data_1[:, i], bins=100, label=\"self\")\n",
    "    ax[i].hist(normalized_data_2[:, i], bins=hist[1], label=\"sklearn\", histtype=\"step\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "scaler = make_pipeline(LogitScaler(), preprocessing.StandardScaler()).fit(data)\n",
    "processed_data = scaler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for i in range(5):\n",
    "    hist = ax[i].hist(processed_data[:, i], bins=100, label=\"logit + scale\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_back = scaler.inverse_transform(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for i in range(5):\n",
    "    hist = ax[i].hist(data[:, i], bins=100, label=\"data\")\n",
    "    ax[i].hist(data_back[:, i], bins=hist[1], label=\"back\", histtype=\"step\")\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "\n",
    "import hydra\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from src.utils.data_generation import generate_data_v2\n",
    "from src.utils.plotting import apply_mpl_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_mpl_styles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = (\n",
    "    \"/beegfs/desy/user/ewencedr/deep-learning/logs/gen_challenge\"\n",
    "    \" logit_true/runs/2023-11-27_01-53-50/\"\n",
    ")\n",
    "# folder = \"/beegfs/desy/user/ewencedr/deep-learning/logs/gen_challenge/runs/2023-11-22_16-03-46/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_backup_file = join(folder, \"config.yaml\")\n",
    "# load everything from experiment config\n",
    "with hydra.initialize(version_base=None, config_path=\"../configs/\"):\n",
    "    if os.path.exists(cfg_backup_file):\n",
    "        print(\"config file already exists --> loading from run directory\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"config file not found\")\n",
    "cfg = OmegaConf.load(cfg_backup_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instantiate model and data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = hydra.utils.instantiate(cfg.data)\n",
    "model = hydra.utils.instantiate(cfg.model)\n",
    "\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = join(folder, \"checkpoints\", \"last-EMA.ckpt\")\n",
    "model = model.load_from_checkpoint(ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_load0_sr = np.load(\n",
    "    \"/beegfs/desy/user/sommerhm/generative_challenge_2023/innerdata_kfold_0.npy\"\n",
    ")\n",
    "data_load1_sr = np.load(\n",
    "    \"/beegfs/desy/user/sommerhm/generative_challenge_2023/innerdata_kfold_1.npy\"\n",
    ")\n",
    "data_load2_sr = np.load(\n",
    "    \"/beegfs/desy/user/sommerhm/generative_challenge_2023/innerdata_kfold_2.npy\"\n",
    ")\n",
    "data_load3_sr = np.load(\n",
    "    \"/beegfs/desy/user/sommerhm/generative_challenge_2023/innerdata_kfold_3.npy\"\n",
    ")\n",
    "data_load4_sr = np.load(\n",
    "    \"/beegfs/desy/user/sommerhm/generative_challenge_2023/innerdata_kfold_4.npy\"\n",
    ")\n",
    "data_load_list_sr = [\n",
    "    data_load0_sr,\n",
    "    data_load1_sr,\n",
    "    data_load2_sr,\n",
    "    data_load3_sr,\n",
    "    data_load4_sr,\n",
    "]\n",
    "innerdata_train = np.concatenate(\n",
    "    [\n",
    "        data_load_list_sr[i]\n",
    "        for i in range(5)\n",
    "        if i not in datamodule.hparams.val_sets + datamodule.hparams.test_sets\n",
    "    ],\n",
    "    axis=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_number = datamodule.hparams.test_sets[0]\n",
    "innerdata_test = np.load(\n",
    "    f\"/beegfs/desy/user/sommerhm/generative_challenge_2023/innerdata_kfold_{test_data_number}.npy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting a KDE for the mass distribution based on the inner training set\n",
    "\n",
    "# we also perform a logit first to stretch out the hard boundaries\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "m_scaler = LogitScaler()\n",
    "m_train = m_scaler.fit_transform(innerdata_train[:, 0:1])\n",
    "\n",
    "kde_model = KernelDensity(bandwidth=0.01, kernel=\"gaussian\")\n",
    "kde_model.fit(m_train)\n",
    "\n",
    "# now let's sample 4x the number of training data\n",
    "m_samples = kde_model.sample(4 * len(m_train)).astype(np.float32)\n",
    "m_samples = m_scaler.inverse_transform(m_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m_samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pre-process mjj samples for use as conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_conditioning = datamodule.preprocessing_pipeline_cond.transform(m_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1111)\n",
    "data, generation_time = generate_data_v2(\n",
    "    model,\n",
    "    num_jet_samples=len(m_conditioning),\n",
    "    batch_size=2048,\n",
    "    cond=torch.Tensor(m_conditioning),\n",
    "    preprocessing_pipeline=datamodule.preprocessing_pipeline,\n",
    "    ode_solver=\"midpoint\",\n",
    "    ode_steps=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.concatenate([m_samples, data], axis=1)  # [:len(innerdata_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Manuel's generated data\n",
    "path_comparison = (\n",
    "    \"/beegfs/desy/user/sommerhm/generative_challenge_2023/no-interpolation_samples.npy\"\n",
    ")\n",
    "data_comparison = np.load(path_comparison)\n",
    "print(data_comparison.shape)\n",
    "samples_comparison = data_comparison[: len(samples)]\n",
    "print(samples_comparison.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing samples to inner background (idealized sanity check)\n",
    "label_map = {\n",
    "    \"0\": r\"$m_{jj}$\",\n",
    "    \"1\": r\"$m_{J_1}$\",\n",
    "    \"2\": r\"$\\Delta m_J$\",\n",
    "    \"3\": r\"$\\tau_{41}^{J_1}$\",\n",
    "    \"4\": r\"$\\tau_{41}^{J_2}$\",\n",
    "}\n",
    "fig, ax = plt.subplots(2, 5, figsize=(35, 8), gridspec_kw={\"height_ratios\": [3, 1]})\n",
    "for i in range(5):\n",
    "    hist_data = ax[0, i].hist(\n",
    "        innerdata_test[:, i], bins=40, label=\"data background\", density=True, histtype=\"stepfilled\"\n",
    "    )\n",
    "    binning = hist_data[1]\n",
    "    next(ax[0, i]._get_lines.prop_cycler)\n",
    "    hist_samples = ax[0, i].hist(\n",
    "        samples[:, i], bins=binning, label=\"sampled background\", density=True, histtype=\"step\"\n",
    "    )\n",
    "    hist_samples_comparison = ax[0, i].hist(\n",
    "        samples_comparison[:, i],\n",
    "        bins=binning,\n",
    "        label=\"sampled background (Manuel)\",\n",
    "        density=True,\n",
    "        histtype=\"step\",\n",
    "    )\n",
    "    # data_hist = hist_data[0]\n",
    "    # sample_hist = hist_samples[0]\n",
    "    data_hist = np.histogram(innerdata_test[:, i], bins=binning, density=False)[0]\n",
    "    sample_hist = np.histogram(samples[:, i], bins=binning, density=False)[0]\n",
    "    sample_hist_comparison = np.histogram(samples_comparison[:, i], bins=binning, density=False)[0]\n",
    "\n",
    "    data_scale_factor = np.sum(data_hist) * np.diff(binning)\n",
    "    sample_scale_factor = np.sum(sample_hist) * np.diff(binning)\n",
    "    sample_scale_factor_comparison = np.sum(sample_hist_comparison) * np.diff(binning)\n",
    "    if i == 2:\n",
    "        ax[0, i].legend(loc=\"best\", frameon=False)\n",
    "    # ax[i].set_ylim(0, plt.gca().get_ylim()[1] * 1.2)\n",
    "    ax[0, i].set_xlabel(f\"{label_map[str(i)]}\")\n",
    "    # ax[0,i].set_yscale(\"log\")\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        ax[1, i].axhline(1.0, color=\"black\", linestyle=\"-\", alpha=0.8)\n",
    "        next(ax[1, i]._get_lines.prop_cycler)\n",
    "        next(ax[1, i]._get_lines.prop_cycler)\n",
    "        ax[1, i].errorbar(\n",
    "            0.5 * (binning[:-1] + binning[1:]),\n",
    "            data_hist / sample_hist * sample_scale_factor / data_scale_factor,\n",
    "            linestyle=\"none\",\n",
    "            marker=\".\",\n",
    "            yerr=np.sqrt(data_hist) / sample_hist * sample_scale_factor / data_scale_factor,\n",
    "        )\n",
    "        ax[1, i].errorbar(\n",
    "            0.5 * (binning[:-1] + binning[1:]),\n",
    "            data_hist\n",
    "            / sample_hist_comparison\n",
    "            * sample_scale_factor_comparison\n",
    "            / data_scale_factor,\n",
    "            linestyle=\"none\",\n",
    "            marker=\".\",\n",
    "            yerr=np.sqrt(data_hist)\n",
    "            / sample_hist_comparison\n",
    "            * sample_scale_factor_comparison\n",
    "            / data_scale_factor,\n",
    "        )\n",
    "\n",
    "        ax[1, i].set_ylim(0.85, 1.15)\n",
    "        # ax[1,i].set_ylim(0.3, 1.7)\n",
    "\n",
    "    if i == 0:\n",
    "        ax[0, i].set_ylabel(\"Events (norm.)\")\n",
    "        ax[1, i].set_ylabel(\"Data/Sample (norm.)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/beegfs/desy/user/ewencedr/data/generative_challenge/gen_data.npy\"\n",
    "save_path_true = \"/beegfs/desy/user/ewencedr/data/generative_challenge/gen_data_true.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(save_path, samples)\n",
    "# np.save(save_path_true, innerdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.load(save_path)\n",
    "innerdata_test = np.load(save_path_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, gridspec_kw={\"height_ratios\": [3, 1]}, sharex=True)\n",
    "    hist_data = ax1.hist(\n",
    "        innerdata_test[:, i], bins=100, label=\"data background\", density=True, histtype=\"step\"\n",
    "    )\n",
    "    binning = hist_data[1]\n",
    "    hist_samples = ax1.hist(\n",
    "        samples[:, i], bins=binning, label=\"sampled background\", density=True, histtype=\"step\"\n",
    "    )\n",
    "    data_hist = hist_data[0]\n",
    "    sample_hist = hist_samples[0]\n",
    "    # ax1.errorbar(0.5*(binning[:-1] + binning[1:]), data_hist/data_scale_factor, yerr=np.sqrt(data_hist)/data_scale_factor, fmt=\"none\", color=data_color)\n",
    "    sample_scale_factor = sum(sample_hist) * np.diff(binning)\n",
    "    data_scale_factor = sum(data_hist) * np.diff(binning)\n",
    "    # print(sample_scale_factor, data_scale_factor)\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        ax2.axhline(1.0, color=\"black\", linestyle=\"-\", alpha=0.8)\n",
    "        ax2.errorbar(\n",
    "            0.5 * (binning[:-1] + binning[1:]),\n",
    "            data_hist / sample_hist * sample_scale_factor / data_scale_factor,\n",
    "            linestyle=\"none\",\n",
    "            marker=\".\",\n",
    "            yerr=np.sqrt(data_hist) / sample_hist * sample_scale_factor / data_scale_factor,\n",
    "        )\n",
    "        ax2.set_ylim(0.85, 1.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_classifier = (\n",
    "    \"/beegfs/desy/user/ewencedr/deep-learning/logs/hl_classifier/runs/2023-12-04_14-45-58/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_backup_file_classifier = join(folder_classifier, \"config.yaml\")\n",
    "# load everything from experiment config\n",
    "with hydra.initialize(version_base=None, config_path=\"../configs/\"):\n",
    "    if os.path.exists(cfg_backup_file_classifier):\n",
    "        print(\"config file already exists --> loading from run directory\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"config file not found\")\n",
    "cfg_classifier = OmegaConf.load(cfg_backup_file_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule_classifier = hydra.utils.instantiate(cfg_classifier.data)\n",
    "model_classifier = hydra.utils.instantiate(cfg_classifier.model)\n",
    "\n",
    "datamodule_classifier.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_classifier = join(folder_classifier, \"checkpoints\", \"last.ckpt\")\n",
    "model_classifier = model_classifier.load_from_checkpoint(ckpt_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_preprocessed = normalize_tensor(\n",
    "    torch.clone(torch.Tensor(samples[:, 1:])),\n",
    "    datamodule_classifier.means,\n",
    "    datamodule_classifier.stds,\n",
    ")\n",
    "samples_preprocessed_comparison = normalize_tensor(\n",
    "    torch.clone(torch.Tensor(samples_comparison[:, 1:])),\n",
    "    datamodule_classifier.means,\n",
    "    datamodule_classifier.stds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(samples_preprocessed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_preds = model_classifier.classify(torch.Tensor(samples_preprocessed)).detach().numpy()\n",
    "classifier_preds_comparison = (\n",
    "    model_classifier.classify(torch.Tensor(samples_preprocessed_comparison)).detach().numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(classifier_preds, bins=100, label=\"samples\")\n",
    "plt.hist(classifier_preds_comparison, bins=100, label=\"samples (Manuel)\", histtype=\"step\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classifier_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshhold_value = 99\n",
    "anomaly_threshold = np.percentile(classifier_preds, threshhold_value)\n",
    "print(anomaly_threshold)\n",
    "selected_samples = samples[classifier_preds > anomaly_threshold]\n",
    "anomaly_threshold_comparison = np.percentile(classifier_preds_comparison, threshhold_value)\n",
    "selected_samples_comparison = samples_comparison[\n",
    "    classifier_preds_comparison > anomaly_threshold_comparison\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing samples to inner background (idealized sanity check)\n",
    "label_map = {\n",
    "    \"0\": r\"$m_{jj}$\",\n",
    "    \"1\": r\"$m_{J_1}$\",\n",
    "    \"2\": r\"$\\Delta m_J$\",\n",
    "    \"3\": r\"$\\tau_{41}^{J_1}$\",\n",
    "    \"4\": r\"$\\tau_{41}^{J_2}$\",\n",
    "}\n",
    "fig, ax = plt.subplots(2, 5, figsize=(35, 8), gridspec_kw={\"height_ratios\": [3, 1]})\n",
    "for i in range(5):\n",
    "    hist_data = ax[0, i].hist(\n",
    "        innerdata_test[:, i], bins=40, label=\"data background\", density=True, histtype=\"stepfilled\"\n",
    "    )\n",
    "    binning = hist_data[1]\n",
    "    next(ax[0, i]._get_lines.prop_cycler)\n",
    "    hist_samples = ax[0, i].hist(\n",
    "        selected_samples[:, i],\n",
    "        bins=binning,\n",
    "        label=\"sampled background\",\n",
    "        density=True,\n",
    "        histtype=\"step\",\n",
    "    )\n",
    "    hist_samples_comparison = ax[0, i].hist(\n",
    "        selected_samples_comparison[:, i],\n",
    "        bins=binning,\n",
    "        label=\"sampled background (Manuel)\",\n",
    "        density=True,\n",
    "        histtype=\"step\",\n",
    "    )\n",
    "    # data_hist = hist_data[0]\n",
    "    # sample_hist = hist_samples[0]\n",
    "    data_hist = np.histogram(innerdata_test[:, i], bins=binning, density=False)[0]\n",
    "    sample_hist = np.histogram(samples[:, i], bins=binning, density=False)[0]\n",
    "    sample_hist_comparison = np.histogram(samples_comparison[:, i], bins=binning, density=False)[0]\n",
    "\n",
    "    data_scale_factor = np.sum(data_hist) * np.diff(binning)\n",
    "    sample_scale_factor = np.sum(sample_hist) * np.diff(binning)\n",
    "    sample_scale_factor_comparison = np.sum(sample_hist_comparison) * np.diff(binning)\n",
    "    if i == 2:\n",
    "        ax[0, i].legend(loc=\"best\", frameon=False)\n",
    "    # ax[i].set_ylim(0, plt.gca().get_ylim()[1] * 1.2)\n",
    "    ax[0, i].set_xlabel(f\"{label_map[str(i)]}\")\n",
    "    ax[0, i].set_yscale(\"log\")\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        ax[1, i].axhline(1.0, color=\"black\", linestyle=\"-\", alpha=0.8)\n",
    "        next(ax[1, i]._get_lines.prop_cycler)\n",
    "        next(ax[1, i]._get_lines.prop_cycler)\n",
    "        ax[1, i].errorbar(\n",
    "            0.5 * (binning[:-1] + binning[1:]),\n",
    "            data_hist / sample_hist * sample_scale_factor / data_scale_factor,\n",
    "            linestyle=\"none\",\n",
    "            marker=\".\",\n",
    "            yerr=np.sqrt(data_hist) / sample_hist * sample_scale_factor / data_scale_factor,\n",
    "        )\n",
    "        ax[1, i].errorbar(\n",
    "            0.5 * (binning[:-1] + binning[1:]),\n",
    "            data_hist\n",
    "            / sample_hist_comparison\n",
    "            * sample_scale_factor_comparison\n",
    "            / data_scale_factor,\n",
    "            linestyle=\"none\",\n",
    "            marker=\".\",\n",
    "            yerr=np.sqrt(data_hist)\n",
    "            / sample_hist_comparison\n",
    "            * sample_scale_factor_comparison\n",
    "            / data_scale_factor,\n",
    "        )\n",
    "\n",
    "        ax[1, i].set_ylim(0.85, 1.15)\n",
    "        # ax[1,i].set_ylim(0.3, 1.7)\n",
    "\n",
    "    if i == 0:\n",
    "        ax[0, i].set_ylabel(\"Events (norm.)\")\n",
    "        ax[1, i].set_ylabel(\"Data/Sample (norm.)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pllhome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

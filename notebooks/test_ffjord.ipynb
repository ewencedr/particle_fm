{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchdyn.core import NeuralODE\n",
    "from torchdyn.datasets import ToyDataset\n",
    "from torchdyn.models import CNF\n",
    "from torchdyn.nn import Augmenter, DataControl, DepthCat\n",
    "\n",
    "# from torchdyn.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick run for automated notebook validation\n",
    "dry_run = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ToyDataset()\n",
    "n_samples = 1 << 14\n",
    "n_gaussians = 7\n",
    "\n",
    "X, yn = data.generate(n_samples, \"diffeqml\", noise=5e-2)\n",
    "X = (X - X.mean()) / X.std()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=\"olive\", alpha=0.3, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "X_train = torch.Tensor(X).to(device)\n",
    "train = data.TensorDataset(X_train)\n",
    "trainloader = data.DataLoader(train, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hutch_trace(x_out, x_in, noise=None, **kwargs):\n",
    "    \"\"\"Hutchinson's trace Jacobian estimator, O(1) call to autograd\"\"\"\n",
    "    jvp = torch.autograd.grad(x_out, x_in, noise, create_graph=True)[0]\n",
    "    trJ = torch.einsum(\"bi,bi->b\", jvp, noise)\n",
    "    return trJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = nn.Sequential(\n",
    "    nn.Linear(2, 64),\n",
    "    nn.Softplus(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.Softplus(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.Softplus(),\n",
    "    nn.Linear(64, 2),\n",
    ")\n",
    "\n",
    "from torch.distributions import (\n",
    "    Categorical,\n",
    "    MultivariateNormal,\n",
    "    SigmoidTransform,\n",
    "    TransformedDistribution,\n",
    "    Uniform,\n",
    ")\n",
    "\n",
    "prior = MultivariateNormal(torch.zeros(2).to(device), torch.eye(2).to(device))\n",
    "\n",
    "# stochastic estimators require a definition of a distribution where \"noise\" vectors are sampled from\n",
    "noise_dist = MultivariateNormal(torch.zeros(2).to(device), torch.eye(2).to(device))\n",
    "# cnf wraps the net as with other energy models\n",
    "cnf = CNF(f, trace_estimator=hutch_trace, noise_dist=noise_dist)\n",
    "nde = NeuralODE(cnf, solver=\"dopri5\", sensitivity=\"adjoint\", atol=1e-4, rtol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(Augmenter(augment_idx=1, augment_dims=1), nde).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "class Learner(pl.LightningModule):\n",
    "    def __init__(self, model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.iters = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self.iters += 1\n",
    "        x = batch[0]\n",
    "        xtrJ = self.model(x)\n",
    "        logprob = (\n",
    "            prior.log_prob(xtrJ[:, 1:]).to(x) - xtrJ[:, 0]\n",
    "        )  # logp(z_S) = logp(z_0) - \\int_0^S trJ\n",
    "        loss = -torch.mean(logprob)\n",
    "        nde.nfe = 0\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.model.parameters(), lr=2e-3, weight_decay=1e-5)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(model)\n",
    "trainer = pl.Trainer(max_epochs=600)\n",
    "trainer.fit(learn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pllhydra",
   "language": "python",
   "name": "pllhydra"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set env variable DATA_DIR again because of hydra\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"DATA_DIR\"] = os.environ.get(\"DATA_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"fm_tops.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load everything from experiment config\n",
    "with hydra.initialize(version_base=None, config_path=\"../configs/\"):\n",
    "    cfg = hydra.compose(config_name=\"train.yaml\", overrides=[f\"experiment={experiment}\"])\n",
    "    # print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = hydra.utils.instantiate(cfg.data)\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetnet.datasets import JetNet\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to one hot encode the jet type and leave the rest of the features as is\n",
    "def OneHotEncodeType(x: np.ndarray):\n",
    "    enc = OneHotEncoder(categories=[[0, 1]])\n",
    "    type_encoded = enc.fit_transform(x[..., 0].reshape(-1, 1)).toarray()\n",
    "    other_features = x[..., 1:].reshape(-1, 3)\n",
    "    return np.concatenate((type_encoded, other_features), axis=-1).reshape(*x.shape[:-1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args = {\n",
    "    # \"jet_type\": [\"g\", \"q\", \"t\", \"w\", \"z\"],  # gluon and top quark jets\n",
    "    \"jet_type\": [\"g\", \"t\"],  # gluon and top quark jets\n",
    "    \"data_dir\": \"/beegfs/desy/user/ewencedr/data/jetnet/\",\n",
    "    # these are the default particle features, written here to be explicit\n",
    "    \"particle_features\": [\"etarel\", \"phirel\", \"ptrel\", \"mask\"],\n",
    "    \"num_particles\": 150,  # we retain only the 10 highest pT particles for this demo\n",
    "    \"jet_features\": [\"type\", \"pt\", \"eta\", \"mass\"],\n",
    "    # we don't want to normalise the 'mask' feature so we set that to False\n",
    "    # \"particle_normalisation\": FeaturewiseLinear(\n",
    "    #    normal=True, normalise_features=[True, True, True, False]\n",
    "    # ),\n",
    "    # pass our function as a transform to be applied to the jet features\n",
    "    \"jet_transform\": OneHotEncodeType,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jets_train = JetNet(**data_args, split=\"train\")\n",
    "jets_valid = JetNet(**data_args, split=\"valid\")\n",
    "jets = JetNet(**data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jets_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_features, jet_features = jets_train[0]\n",
    "print(f\"Particle features ({data_args['particle_features']}):\\n\\t{particle_features}\")\n",
    "print(f\"\\nJet features ({data_args['jet_features']}):\\n\\t{jet_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args_np = {\n",
    "    \"jet_type\": [\"g\", \"q\", \"t\", \"z\"],  # gluon and top quark jets\n",
    "    # \"jet_type\": [\"z\"],  # gluon and top quark jets\n",
    "    \"data_dir\": \"/beegfs/desy/user/ewencedr/data/jetnet/\",\n",
    "    # these are the default particle features, written here to be explicit\n",
    "    \"particle_features\": [\"etarel\", \"phirel\", \"ptrel\", \"mask\"],\n",
    "    \"num_particles\": 150,  # we retain only the 10 highest pT particles for this demo\n",
    "    \"jet_features\": [\"type\", \"pt\", \"eta\", \"mass\"],\n",
    "    # we don't want to normalise the 'mask' feature so we set that to False\n",
    "    # \"particle_normalisation\": FeaturewiseLinear(\n",
    "    #    normal=True, normalise_features=[True, True, True, False]\n",
    "    # ),\n",
    "    # pass our function as a transform to be applied to the jet features\n",
    "    # \"jet_transform\": OneHotEncodeType,\n",
    "    \"split\": \"all\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_data_np, jet_data_np = JetNet.getData(**data_args_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(particle_data_np.shape)\n",
    "print(jet_data_np[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gluon: 0\n",
    "# quark: 1\n",
    "# top: 2\n",
    "# w: 3\n",
    "# z: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_dict = {\"g\": 0, \"q\": 1, \"t\": 2, \"w\": 3, \"z\": 4}\n",
    "categories = []\n",
    "for type in data_args_np[\"jet_type\"]:\n",
    "    categories.append(type_dict[type])\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneHotEncodeTypeNp(x: np.ndarray, categories: list = [[0, 1, 2, 3, 4]]):\n",
    "    \"\"\"One hot encode the jet type and leave the rest of the features as is\n",
    "        Note: The one_hot encoded value is based on the position in the categories list not the value itself,\n",
    "        e.g. categories: [0,3] results in the two one_hot encoded values [1,0] and [0,1]\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): jet data with shape (num_jets, num_features) that contains the jet type in the first column\n",
    "        categories (list, optional): List with values in x that should be one hot encoded. Defaults to [[0, 1, 2, 3, 4]].\n",
    "\n",
    "    Returns:\n",
    "        np.array: one_hot_encoded jet data (num_jets, num_features) with feature length len(categories) + 3 (pt, eta, mass)\n",
    "    \"\"\"\n",
    "    enc = OneHotEncoder(categories=categories)\n",
    "    type_encoded = enc.fit_transform(x[..., 0].reshape(-1, 1)).toarray()\n",
    "    other_features = x[..., 1:].reshape(-1, 3)\n",
    "    return np.concatenate((type_encoded, other_features), axis=-1).reshape(*x.shape[:-1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_data_one_hot = OneHotEncodeTypeNp(jet_data_np, categories=[categories])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jet_data_one_hot.shape)\n",
    "print(jet_data_one_hot[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioning_type = True\n",
    "conditioning_pt = False\n",
    "conditioning_eta = False\n",
    "conditioning_mass = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_len = len(categories)\n",
    "print(one_hot_len)\n",
    "keep_col = []\n",
    "if conditioning_type:\n",
    "    keep_col.append(np.arange(one_hot_len))\n",
    "if conditioning_pt:\n",
    "    keep_col.append(np.arange(one_hot_len, one_hot_len + 1))\n",
    "if conditioning_eta:\n",
    "    keep_col.append(np.arange(one_hot_len + 1, one_hot_len + 2))\n",
    "if conditioning_mass:\n",
    "    keep_col.append(np.arange(one_hot_len + 2, one_hot_len + 3))\n",
    "keep_col = np.concatenate(keep_col)\n",
    "print(keep_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what happens if no conditioning is used?\n",
    "jet_data_final = jet_data_one_hot[..., keep_col]\n",
    "print(jet_data_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jet_data_one_hot.shape)\n",
    "print(jet_data_one_hot[:, [0, 1, 2, 5]].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = np.array([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
    "test_array = np.repeat(test_array, 10, axis=0)\n",
    "print(test_array.shape)\n",
    "print(test_array)\n",
    "print(test_array[:, [0, 1, 2, 5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(datamodule.tensor_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color: str = (\"#E2001A\",)\n",
    "mask_data = np.ma.masked_where(\n",
    "    data[:, :, 0] == 0,\n",
    "    data[:, :, 0],\n",
    ")\n",
    "mask = np.expand_dims(mask_data, axis=-1)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "gs = GridSpec(1, 1)\n",
    "ax = fig.add_subplot(gs[0])\n",
    "# idx = np.random.randint(len(data))\n",
    "for idx in tqdm(range(1000)):\n",
    "    x_plot = data[idx, :, :2]  # .cpu()\n",
    "    s_plot = np.abs(data[idx, :, 2])  # .cpu())\n",
    "    s_plot[mask[idx, :, 0] < 0.0] = 0.0\n",
    "\n",
    "    ax.scatter(*x_plot.T, s=50 * s_plot, color=color, alpha=0.5)\n",
    "\n",
    "ax.set_xlabel(r\"$\\eta$\")\n",
    "ax.set_ylabel(r\"$\\phi$\")\n",
    "\n",
    "ax.set_xlim(-0.3, 0.3)\n",
    "ax.set_ylim(-0.3, 0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single_jets(\n",
    "    data: np.ndarray,\n",
    "    color: str = \"#E2001A\",\n",
    "    save_folder: str = \"logs/\",\n",
    "    save_name: str = \"sim_jets\",\n",
    ") -> plt.figure:\n",
    "    \"\"\"Create a plot with 16 randomly selected jets from the data.\n",
    "\n",
    "    Args:\n",
    "        data (_type_): Data to plot.\n",
    "        color (str, optional): Color of plotted point cloud. Defaults to \"#E2001A\".\n",
    "        save_folder (str, optional): Path to folder where the plot is saved. Defaults to \"logs/\".\n",
    "        save_name (str, optional): File_name for saving the plot. Defaults to \"sim_jets\".\n",
    "    \"\"\"\n",
    "    mask_data = np.ma.masked_where(\n",
    "        data[:, :, 0] == 0,\n",
    "        data[:, :, 0],\n",
    "    )\n",
    "    mask = np.expand_dims(mask_data, axis=-1)\n",
    "    fig = plt.figure(figsize=(16, 16))\n",
    "    gs = GridSpec(4, 4)\n",
    "\n",
    "    for i in tqdm(range(16)):\n",
    "        ax = fig.add_subplot(gs[i])\n",
    "\n",
    "        idx = np.random.randint(len(data))\n",
    "        x_plot = data[idx, :, :2]  # .cpu()\n",
    "        s_plot = np.abs(data[idx, :, 2])  # .cpu())\n",
    "        s_plot[mask[idx, :, 0] < 0.0] = 0.0\n",
    "\n",
    "        ax.scatter(*x_plot.T, s=5000 * s_plot, color=color, alpha=0.5)\n",
    "\n",
    "        ax.set_xlabel(r\"$\\eta$\")\n",
    "        ax.set_ylabel(r\"$\\phi$\")\n",
    "\n",
    "        ax.set_xlim(-0.3, 0.3)\n",
    "        ax.set_ylim(-0.3, 0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f\"{save_folder}{save_name}.png\", bbox_inches=\"tight\")\n",
    "    return fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Sliced Wasserstein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swd(data: torch.Tensor, preds: torch.Tensor, n_proj: int = 1024) -> torch.Tensor:\n",
    "    \"\"\"Sliced Wassersteini Distance\n",
    "    Compute the Wasserstein distance between two point clouds.\n",
    "    Inspired by https://github.com/apple/ml-cvpr2019-swd/blob/master/swd.py#L45\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor) [batch, n_points, feats]: Ground truth.\n",
    "        preds (torch.Tensor) [batch, n_points, feats]: Predictions.\n",
    "        n_proj (int, optional): number of random 1d projections. Defaults to 1024.\n",
    "\n",
    "    Returns:\n",
    "        wdist (torch.Tensor) [1]: Wasserstein distance\n",
    "    \"\"\"\n",
    "\n",
    "    b, p, f = data.shape  # [batch,points,feats]\n",
    "    data, preds = data.float(), preds.float()\n",
    "    proj = torch.randn(f, n_proj, device=data.device)  # [feats, l]\n",
    "    print(f\"proj: {proj.shape}\")\n",
    "    proj *= torch.rsqrt(torch.sum(torch.square(proj), 0, keepdim=True))\n",
    "    print(f\"proj: {proj.shape}\")\n",
    "    proj = proj.view(1, f, n_proj).expand(b, -1, -1)  # first add dim, then expand to batch dim\n",
    "    print(f\"proj: {proj.shape}\")\n",
    "    p1 = torch.matmul(data, proj)  # shape: [batch, n_points, l]\n",
    "    print(f\"p1: {p1.shape}\")\n",
    "    p2 = torch.matmul(preds, proj)  # shape: [batch, n_points, l]\n",
    "    print(f\"p2: {p2.shape}\")\n",
    "    p1, _ = torch.sort(p1, 1, descending=True)  # point wise sorting\n",
    "    print(f\"p1: {p1.shape}\")\n",
    "    p2, _ = torch.sort(p2, 1, descending=True)\n",
    "    print(f\"p2: {p2.shape}\")\n",
    "    wdist = torch.mean(torch.square(p1 - p2))  # MSE\n",
    "    return wdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1 = torch.tensor(data)\n",
    "tensor2 = torch.tensor(data) + 0.1\n",
    "print(tensor1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swd_ = swd(tensor1, tensor2)\n",
    "print(swd_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pllhome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

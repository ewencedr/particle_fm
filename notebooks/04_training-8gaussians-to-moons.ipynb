{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd0b73a3",
   "metadata": {},
   "source": [
    "###### based on https://github.com/atong01/conditional-flow-matching/blob/main/notebooks/training-8gaussians-to-moons.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9525284d-82d9-4e71-9825-953fcc43fe9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conditional Flow Matching\n",
    "\n",
    "This notebook is a self-contained example of conditional flow matching. We implement a number of different simulation-free methods for learning flow models. They differ based on the interpolant used and the loss function used to train them.\n",
    "\n",
    "In this notebook we implement 5 models that can map from a source distribution $q_0$ to a target distribution $q_1$:\n",
    "* Conditional Flow Matching (CFM)\n",
    "    * This is equivalent to the basic (non-rectified) formulation of \"Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow\" [(Liu et al. 2023)](https://openreview.net/forum?id=XVjTT1nw5z)\n",
    "    * Is similar to \"Stochastic Interpolants\" [(Albergo et al. 2023)](https://openreview.net/forum?id=li7qeBbCR1t) with a non-variance preserving interpolant.\n",
    "    * Is similar to \"Flow Matching\" [(Lipman et al. 2023)](https://openreview.net/forum?id=PqvMRDCJT9t) but conditions on both source and target.\n",
    "* Optimal Transport CFM (OT-CFM), which directly optimizes for dynamic optimal transport\n",
    "* Schrödinger Bridge CFM (SB-CFM), which optimizes for Schrödinger Bridge probability paths\n",
    "* \"Building Normalizing Flows with Stochastic Interpolants\" [(Albergo et al. 2023)](https://openreview.net/forum?id=li7qeBbCR1t) this corresponds to \"VP-CFM\" in our README referring to its variance preserving properties.\n",
    "* \"Action Matching: Learning Stochastic Dynamics From Samples\" [(Neklyudov et al. 2022)](https://arxiv.org/abs/2210.06662)\n",
    "\n",
    "Note that this Flow Matching is different from the Generative Flow Network Flow Matching losses. Here we specifically regress against continuous flows, rather than matching inflows and outflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89efbbe0-6c0a-4b92-873c-0184450e5a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ot as pot\n",
    "import torch\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# import torchdyn\n",
    "from torchdyn.core import NeuralODE\n",
    "from torchdyn.datasets import generate_moons\n",
    "\n",
    "savedir = \"models/8gaussian-moons\"\n",
    "os.makedirs(savedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a83e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from src.models.components import EPiC_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489aa1a0-5caa-43a9-b813-74fadba4953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement some helper functions\n",
    "\n",
    "\n",
    "def eight_normal_sample(n, dim, scale=1, var=1):\n",
    "    m = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "        torch.zeros(dim), math.sqrt(var) * torch.eye(dim)\n",
    "    )\n",
    "    centers = [\n",
    "        (1, 0),\n",
    "        (-1, 0),\n",
    "        (0, 1),\n",
    "        (0, -1),\n",
    "        (1.0 / np.sqrt(2), 1.0 / np.sqrt(2)),\n",
    "        (1.0 / np.sqrt(2), -1.0 / np.sqrt(2)),\n",
    "        (-1.0 / np.sqrt(2), 1.0 / np.sqrt(2)),\n",
    "        (-1.0 / np.sqrt(2), -1.0 / np.sqrt(2)),\n",
    "    ]\n",
    "    centers = torch.tensor(centers) * scale\n",
    "    noise = m.sample((n,))\n",
    "    multi = torch.multinomial(torch.ones(8), n, replacement=True)\n",
    "    data = []\n",
    "    for i in range(n):\n",
    "        data.append(centers[multi[i]] + noise[i])\n",
    "    data = torch.stack(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def sample_moons(n, num_points=30):\n",
    "    # x0, _ = generate_moons(n, noise=0.2)\n",
    "    # return x0 * 3 - 1\n",
    "    data = []\n",
    "    for _ in range(n):\n",
    "        x, _ = make_moons(n_samples=num_points, noise=0.05, shuffle=True)\n",
    "        data.append(x)\n",
    "    return torch.tensor(np.array(data), dtype=torch.float32) * 3 - 1\n",
    "\n",
    "\n",
    "def sample_8gaussians(n, num_points=30):\n",
    "    data = []\n",
    "    for _ in range(n):\n",
    "        x = eight_normal_sample(num_points, 2, scale=5, var=0.1).float()\n",
    "        data.append(np.array(x))\n",
    "    return torch.tensor(np.array(data), dtype=torch.float32)\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, dim, out_dim=None, w=64, time_varying=False):\n",
    "        super().__init__()\n",
    "        self.time_varying = time_varying\n",
    "        if out_dim is None:\n",
    "            out_dim = dim\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(dim + (1 if time_varying else 0), w),\n",
    "            torch.nn.SELU(),\n",
    "            torch.nn.Linear(w, w),\n",
    "            torch.nn.SELU(),\n",
    "            torch.nn.Linear(w, w),\n",
    "            torch.nn.SELU(),\n",
    "            torch.nn.Linear(w, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class GradModel(torch.nn.Module):\n",
    "    def __init__(self, action):\n",
    "        super().__init__()\n",
    "        self.action = action\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.requires_grad_(True)\n",
    "        grad = torch.autograd.grad(torch.sum(self.action(x)), x, create_graph=True)[0]\n",
    "        return grad[:, :-1]\n",
    "\n",
    "\n",
    "class torch_wrapper(torch.nn.Module):\n",
    "    \"\"\"Wraps model to torchdyn compatible format.\"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        return self.model(torch.cat([x, t.repeat(x.shape[:-1])[..., None]], dim=-1))\n",
    "\n",
    "\n",
    "class torch_wrapper_epic(torch.nn.Module):\n",
    "    \"\"\"Wraps model to torchdyn compatible format.\"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        x_local = torch.cat([x, t.repeat(x.shape[:-1])[..., None]], dim=-1)\n",
    "        x_global = torch.randn_like(torch.ones(x_local.shape[0], 16, device=x_local.device))\n",
    "        return self.model(t, x_global, x_local)\n",
    "\n",
    "\n",
    "def plot_trajectories(traj):\n",
    "    n = 2000\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(traj[0, :n, :, 0], traj[0, :n, :, 1], s=10, alpha=0.8, c=\"black\")\n",
    "    plt.scatter(traj[:, :n, :, 0], traj[:, :n, :, 1], s=0.2, alpha=0.2, c=\"olive\")\n",
    "    plt.scatter(traj[-1, :n, :, 0], traj[-1, :n, :, 1], s=4, alpha=1, c=\"blue\")\n",
    "    plt.legend([\"Prior sample z(S)\", \"Flow\", \"z(0)\"])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c8135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "moons = sample_moons(100)\n",
    "print(moons.shape)\n",
    "gaussians = sample_8gaussians(100)\n",
    "print(gaussians.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ab43d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor(0.0)\n",
    "rep = t.repeat(moons.shape[:-1])[..., None]\n",
    "cat = torch.cat([moons, rep], dim=-1)\n",
    "print(cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1297ade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(moons[:10, :, 0], moons[:10, :, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecdb0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(gaussians[:10, :, 0], gaussians[:10, :, 1])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5b87ac5-6349-4dfb-8b17-eaa3940d3935",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Conditional Flow Matching\n",
    "\n",
    "First we implement the basic conditional flow matching. As in the paper, we have\n",
    "$$\n",
    "\\begin{align}\n",
    "z &= (x_0, x_1) \\\\\n",
    "q(z) &= q(x_0)q(x_1) \\\\\n",
    "p_t(x | z) &= \\mathcal{N}(x | t * x_1 + (1 - t) * x_0, \\sigma^2) \\\\\n",
    "u_t(x | z) &= x_1 - x_0\n",
    "\\end{align}\n",
    "$$\n",
    "When $\\sigma = 0$ this is equivalent to zero-steps of rectified flow. We find that small $\\sigma$ helps to regularize the problem ymmv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00283979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213f12bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_set_feats(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    if len(tensor.shape) != 3:\n",
    "        raise ValueError(\"Input tensor must be of shape (batch, set_size, feats)\")\n",
    "    mean_set = torch.mean(tensor, dim=-2)\n",
    "    sum_set = torch.sum(tensor, dim=-2)\n",
    "    mean_feat = torch.mean(tensor, dim=-1)\n",
    "    mean_set_feat = torch.mean(mean_feat, dim=-1).unsqueeze(-1)\n",
    "    sum_set_feat = torch.sum(mean_feat, dim=-1).unsqueeze(-1)\n",
    "    cat = torch.cat([mean_set, sum_set, mean_set_feat, sum_set_feat], dim=-1)\n",
    "    return cat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28db32ae",
   "metadata": {},
   "source": [
    "## Lipman Flow Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bda5eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "steps = 500\n",
    "sigma = 1e-4\n",
    "dim = 2\n",
    "batch_size = 1024\n",
    "# model = MLP(dim=dim, time_varying=True)\n",
    "model = EPiC_generator(feats=dim, input_dim=dim + 1)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "start = time.time()\n",
    "data = sample_moons(batch_size * 10)  # data\n",
    "for k in tqdm(range(steps)):\n",
    "    optimizer.zero_grad()\n",
    "    # x = sample_8gaussians(batch_size)\n",
    "\n",
    "    subset = torch.randint(0, len(data), (batch_size,))\n",
    "    x = data[subset]\n",
    "\n",
    "    # print(f\"X shape: {x.shape}\")\n",
    "    t = torch.rand_like(torch.ones(x.shape[0]))\n",
    "    t = t.unsqueeze(-1).repeat_interleave(x.shape[1], dim=1).unsqueeze(-1)\n",
    "    t = t.type_as(x)\n",
    "    # print(f\"t shape: {t.shape}\")\n",
    "    # print(f\"t shape: {t[:2]}\")\n",
    "\n",
    "    z = torch.randn_like(x)\n",
    "    # print(f\"z shape: {z.shape}\")\n",
    "\n",
    "    y = (1 - t) * x + (sigma + (1 - sigma) * t) * z\n",
    "    # print(f\"y shape: {y.shape}\")\n",
    "    ut = (1 - sigma) * z - x\n",
    "    # print(f\"ut shape: {ut.shape}\")\n",
    "    # frequencies = 6\n",
    "    # print(f\"t shape: {t.shape}\")\n",
    "    # print(f\"tsqueeze shape: {t.squeeze().shape}\")\n",
    "    # t = t.squeeze(-1)\n",
    "    # print(f\"t shape: {t.shape}\")\n",
    "    # t = frequencies * t[..., None]\n",
    "    # print(f\"t shape: {t.shape}\")\n",
    "    # t = torch.cat((t.cos(), t.sin()), dim=-1)\n",
    "    # print(f\"t shape: {t.shape}\")\n",
    "    # t = t.expand(*x.shape[:-1], -1)\n",
    "    # print(f\"t shape: {t.shape}\")\n",
    "    x_local = torch.cat([x, t], dim=-1)\n",
    "    x_global = torch.randn_like(torch.ones(x_local.shape[0], 16, device=x_local.device))\n",
    "    vt = model(t, x_global, x_local)\n",
    "\n",
    "    loss = torch.mean((vt - ut) ** 2)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # if (k + 1) % 30 == 0:\n",
    "    #    print(f\"k: {k+1}\")\n",
    "    if (k + 1) % 100 == 0:\n",
    "        end = time.time()\n",
    "        print(f\"{k+1}: loss {loss.item():0.3f} time {(end - start):0.2f}\")\n",
    "        start = end\n",
    "        node = NeuralODE(\n",
    "            torch_wrapper_epic(model),\n",
    "            solver=\"midpoint\",\n",
    "            sensitivity=\"adjoint\",\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            traj = node.trajectory(\n",
    "                torch.randn_like(x)[:100],\n",
    "                t_span=torch.linspace(1.0, 0.0, 50),\n",
    "            )\n",
    "            plt.scatter(traj[-1:, :, 0], traj[-1:, :, 1])\n",
    "            plt.show()\n",
    "            plot_trajectories(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57227672",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:, :, 0], data[:, :, 1])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbf4eb90",
   "metadata": {},
   "source": [
    "## Conditional Flow Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0717857c-e4a2-4dbe-88d1-b2399b4eab37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "steps = 500\n",
    "sigma = 0.1\n",
    "dim = 2\n",
    "batch_size = 256\n",
    "# model = MLP(dim=dim, time_varying=True)\n",
    "model = EPiC_generator(feats=dim, input_dim=dim + 1)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "start = time.time()\n",
    "for k in tqdm(range(steps)):\n",
    "    optimizer.zero_grad()\n",
    "    x0 = sample_8gaussians(batch_size)\n",
    "    x1 = sample_moons(batch_size)\n",
    "\n",
    "    t = torch.rand_like(torch.ones(x0.shape[0]))\n",
    "    t = t.unsqueeze(-1).repeat_interleave(x0.shape[1], dim=1).unsqueeze(-1)\n",
    "    t = t.type_as(x0)\n",
    "\n",
    "    mu_t = (1 - t) * x1 + t * x0\n",
    "\n",
    "    sigma_t = sigma\n",
    "    x = mu_t + sigma_t * torch.randn_like(x0)\n",
    "\n",
    "    ut = x0 - x1\n",
    "\n",
    "    x_local = torch.cat([x, (t)], dim=-1)\n",
    "    x_global = torch.randn_like(torch.ones(x_local.shape[0], 16, device=x_local.device))\n",
    "    vt = model(t, x_global, x_local)\n",
    "\n",
    "    loss = torch.mean((vt - ut) ** 2)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # if (k + 1) % 30 == 0:\n",
    "    #    print(f\"k: {k+1}\")\n",
    "    if (k + 1) % 100 == 0:\n",
    "        end = time.time()\n",
    "        print(f\"{k+1}: loss {loss.item():0.3f} time {(end - start):0.2f}\")\n",
    "        start = end\n",
    "        node = NeuralODE(\n",
    "            torch_wrapper_epic(model),\n",
    "            solver=\"midpoint\",\n",
    "            sensitivity=\"adjoint\",\n",
    "            atol=1e-4,\n",
    "            rtol=1e-4,\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            traj = node.trajectory(\n",
    "                sample_8gaussians(100),\n",
    "                t_span=torch.linspace(1.0, 0.0, 50),\n",
    "            )\n",
    "            plot_trajectories(traj)\n",
    "# torch.save(model, f\"{savedir}/cfm_v1.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dcb7bd61",
   "metadata": {},
   "source": [
    "### Optimal Transport Conditional Flow Matching\n",
    "\n",
    "Next we implement optimal transport conditional flow matching. As in the paper, here we have\n",
    "$$\n",
    "\\begin{align}\n",
    "z &= (x_0, x_1) \\\\\n",
    "q(z) &= \\pi(x_0, x_1) \\\\\n",
    "p_t(x | z) &= \\mathcal{N}(x | t * x_1 + (1 - t) * x_0, \\sigma^2) \\\\\n",
    "u_t(x | z) &= x_1 - x_0\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\pi$ is the joint of an exact optimal transport matrix. We first sample random $x_0, x_1$, then resample according to the optimal transport matrix as computed with the python optimal transport package. We use the 2-Wasserstein distance with an $L^2$ ground distance for equivalence with dynamic optimal transport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d1a455",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sigma = 0.1\n",
    "dim = 2\n",
    "batch_size = 256\n",
    "# model = MLP(dim=dim, time_varying=True)\n",
    "model = EPiC_generator(feats=dim, latent_local=dim + 1)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "start = time.time()\n",
    "for z in tqdm(range(500)):\n",
    "    optimizer.zero_grad()\n",
    "    x0 = sample_8gaussians(batch_size)  # (x,y)\n",
    "    x1 = sample_moons(batch_size)  # (x,y)\n",
    "    print(f\"x_0: {x0.shape}\")\n",
    "    # print(f\"x_1: {x1.shape}\")\n",
    "\n",
    "    t = torch.rand_like(torch.ones(x0.shape[0]))\n",
    "    t = t.unsqueeze(-1).repeat_interleave(x0.shape[1], dim=1).unsqueeze(-1)\n",
    "    t = t.type_as(x0)\n",
    "\n",
    "    a, b = pot.unif(x0.size()[1]), pot.unif(x1.size()[1])\n",
    "    a = np.repeat(np.expand_dims(a, axis=0), x0.size()[0], axis=0)\n",
    "    b = np.repeat(np.expand_dims(b, axis=0), x1.size()[0], axis=0)\n",
    "    # print(f\"a: {a}\")\n",
    "    M = torch.cdist(x0, x1) ** 2\n",
    "    # M = M / M.max()\n",
    "    # M2 = torch.nn.functional.normalize(M, dim=1)\n",
    "    # print(f\"M2: {M2.shape}\")\n",
    "    # print(f\"M: {M.shape}\")\n",
    "    # print(M2-M)\n",
    "    # print(f\"a.shape: {a.shape}\")\n",
    "    # print(f\"M.shape: {M.shape}\")\n",
    "    # pi = []\n",
    "    # print(f\"x0: {x0.shape}\")\n",
    "    # print(f\"x1: {x1.shape}\")\n",
    "    for k in range(M.shape[0]):\n",
    "        M[k] = M[k] / M[k].max()\n",
    "        pi = pot.emd(a[k], b[k], M[k].detach().cpu().numpy())\n",
    "        print(f\"pi: {pi.shape}\")\n",
    "        p = pi.flatten()\n",
    "        print(f\"p: {p.shape}\")\n",
    "        p = p / p.sum()\n",
    "        choices = np.random.choice(pi.shape[0] * pi.shape[1], p=p, size=pi.shape[0])\n",
    "        print(f\"choices: {choices.shape}\")\n",
    "        i, j = np.divmod(choices, pi.shape[1])\n",
    "        print(f\"i   : {i}\")\n",
    "        print(f\"j   : {j}\")\n",
    "        x0[k] = x0[k, i]\n",
    "        x1[k] = x1[k, j]\n",
    "        # pi.append(pi_temp)\n",
    "\n",
    "    # pi = np.array(pi)\n",
    "    # print(f\"pi: {pi.shape}\")\n",
    "\n",
    "    # Sample random interpolations on pi\n",
    "    # p = pi.reshape(pi.shape[0], -1)\n",
    "    # p = p / p.sum()\n",
    "    # choices = []\n",
    "    # for i in range(p.shape[0]):\n",
    "    #    p[i] = p[i] / p[i].sum()\n",
    "    #    choices_temp = np.random.choice(pi.shape[1] * pi.shape[2], p=p[i], size=pi.shape[1])\n",
    "    #    choices.append(choices_temp)\n",
    "    # choices = np.array(choices)\n",
    "    # print(f\"choices: {choices.shape}\")\n",
    "\n",
    "    # i, j = np.divmod(choices, pi.shape[2])\n",
    "    # print(f\"i: {i.shape}\")\n",
    "    # print(f\"x0: {x0.shape}\")\n",
    "    #\n",
    "    # for k in range(i.shape[0]):\n",
    "    #    x0[k] = x0[k][i[k]]\n",
    "    #    x1[k] = x1[k][j[k]]\n",
    "    # x0 = x0[i]\n",
    "    # x1 = x1[j]\n",
    "    # print(\"test\")\n",
    "    # calculate regression loss\n",
    "    # print(f\"x0: {x0.shape}\")\n",
    "    # print(f\"x1: {x1.shape}\")\n",
    "    # print(f\"t: {t.shape}\")\n",
    "    mu_t = x0 * t + x1 * (1 - t)\n",
    "    # print(f\"mu_t: {mu_t.shape}\")\n",
    "    sigma_t = sigma\n",
    "    x = mu_t + sigma_t * torch.randn_like(x0)\n",
    "    ut = x0 - x1\n",
    "    x_local = torch.cat([x, t], dim=-1)\n",
    "    # print(f\"x_local: {x_local.shape}\")\n",
    "    x_global = torch.randn_like(torch.ones(x_local.shape[0], 16, device=x_local.device))\n",
    "    vt = model(t, x_global, x_local)\n",
    "    loss = torch.mean((vt - ut) ** 2)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # if (z + 1) % 30 == 0:\n",
    "    #    print(f\"z: {z+1}\")\n",
    "    if (z + 1) % 100 == 0:\n",
    "        end = time.time()\n",
    "        print(f\"{z+1}: loss {loss.item():0.3f} time {(end - start):0.2f}\")\n",
    "        start = end\n",
    "        node = NeuralODE(\n",
    "            torch_wrapper_epic(model),\n",
    "            solver=\"midpoint\",\n",
    "            sensitivity=\"adjoint\",\n",
    "            atol=1e-4,\n",
    "            rtol=1e-4,\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            traj = node.trajectory(\n",
    "                sample_8gaussians(100),\n",
    "                t_span=torch.linspace(1, 0, 50),\n",
    "            )\n",
    "            plot_trajectories(traj)\n",
    "# torch.save(model, f\"{savedir}/cfm_v1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78e57c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(traj.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa8051ab",
   "metadata": {},
   "source": [
    "old slower version\n",
    "\n",
    "time the other way around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930b63a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sigma = 0.1\n",
    "dim = 2\n",
    "batch_size = 256\n",
    "# model = MLP(dim=dim, time_varying=True)\n",
    "model = EPiC_generator(feats=dim, latent_local=dim + 1)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "start = time.time()\n",
    "for z in tqdm(range(500)):\n",
    "    optimizer.zero_grad()\n",
    "    x0 = sample_8gaussians(batch_size)  # (x,y)\n",
    "    x1 = sample_moons(batch_size)  # (x,y)\n",
    "    # print(f\"x_0: {x0.shape}\")\n",
    "    # print(f\"x_1: {x1.shape}\")\n",
    "\n",
    "    t = torch.rand_like(torch.ones(x0.shape[0]))\n",
    "    t = t.unsqueeze(-1).repeat_interleave(x0.shape[1], dim=1).unsqueeze(-1)\n",
    "    t = t.type_as(x0)\n",
    "\n",
    "    a, b = pot.unif(x0.size()[1]), pot.unif(x1.size()[1])\n",
    "    a = np.repeat(np.expand_dims(a, axis=0), x0.size()[0], axis=0)\n",
    "    b = np.repeat(np.expand_dims(b, axis=0), x1.size()[0], axis=0)\n",
    "    # print(f\"a: {a}\")\n",
    "    M = torch.cdist(x0, x1) ** 2\n",
    "    # M = M / M.max()\n",
    "    # M2 = torch.nn.functional.normalize(M, dim=1)\n",
    "    # print(f\"M2: {M2.shape}\")\n",
    "    # print(f\"M: {M.shape}\")\n",
    "    # print(M2-M)\n",
    "    # print(f\"a.shape: {a.shape}\")\n",
    "    # print(f\"M.shape: {M.shape}\")\n",
    "    pi = []\n",
    "    for k in range(M.shape[0]):\n",
    "        M[k] = M[k] / M[k].max()\n",
    "        pi_temp = pot.emd(a[k], b[k], M[k].detach().cpu().numpy())\n",
    "\n",
    "        pi.append(pi_temp)\n",
    "\n",
    "    pi = np.array(pi)\n",
    "    # print(f\"pi: {pi.shape}\")\n",
    "\n",
    "    # Sample random interpolations on pi\n",
    "    p = pi.reshape(pi.shape[0], -1)\n",
    "    # p = p / p.sum()\n",
    "    choices = []\n",
    "    for i in range(p.shape[0]):\n",
    "        p[i] = p[i] / p[i].sum()\n",
    "        choices_temp = np.random.choice(pi.shape[1] * pi.shape[2], p=p[i], size=pi.shape[1])\n",
    "        choices.append(choices_temp)\n",
    "    choices = np.array(choices)\n",
    "    # print(f\"choices: {choices.shape}\")\n",
    "\n",
    "    i, j = np.divmod(choices, pi.shape[2])\n",
    "    # print(f\"i: {i.shape}\")\n",
    "    # print(f\"x0: {x0.shape}\")\n",
    "    #\n",
    "    for k in range(i.shape[0]):\n",
    "        x0[k] = x0[k][i[k]]\n",
    "        x1[k] = x1[k][j[k]]\n",
    "    # x0 = x0[i]\n",
    "    # x1 = x1[j]\n",
    "    # print(\"test\")\n",
    "    # calculate regression loss\n",
    "    # print(f\"x0: {x0.shape}\")\n",
    "    # print(f\"x1: {x1.shape}\")\n",
    "    # print(f\"t: {t.shape}\")\n",
    "    mu_t = x0 * (1 - t) + x1 * t\n",
    "    # print(f\"mu_t: {mu_t.shape}\")\n",
    "    sigma_t = sigma\n",
    "    x = mu_t + sigma_t * torch.randn_like(x0)\n",
    "    ut = x1 - x0\n",
    "    x_local = torch.cat([x, t], dim=-1)\n",
    "    # print(f\"x_local: {x_local.shape}\")\n",
    "    x_global = torch.randn_like(torch.ones(x_local.shape[0], 16, device=x_local.device))\n",
    "    vt = model(t, x_global, x_local)\n",
    "    loss = torch.mean((vt - ut) ** 2)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # if (z + 1) % 30 == 0:\n",
    "    #    print(f\"z: {z+1}\")\n",
    "    if (z + 1) % 100 == 0:\n",
    "        end = time.time()\n",
    "        print(f\"{z+1}: loss {loss.item():0.3f} time {(end - start):0.2f}\")\n",
    "        start = end\n",
    "        node = NeuralODE(\n",
    "            torch_wrapper_epic(model),\n",
    "            solver=\"midpoint\",\n",
    "            sensitivity=\"adjoint\",\n",
    "            atol=1e-4,\n",
    "            rtol=1e-4,\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            traj = node.trajectory(\n",
    "                sample_8gaussians(100),\n",
    "                t_span=torch.linspace(0, 1, 50),\n",
    "            )\n",
    "            plot_trajectories(traj)\n",
    "# torch.save(model, f\"{savedir}/cfm_v1.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2cbe39cb",
   "metadata": {},
   "source": [
    "### plot single trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc27451",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(traj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77600a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectories1(traj):\n",
    "    n = 1\n",
    "    m = 2\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(traj[0, :n, :, 0], traj[0, :n, :, 1], s=10, alpha=0.8, c=\"black\")\n",
    "    plt.scatter(traj[:, :n, :, 0], traj[:, :n, :, 1], s=0.2, alpha=0.2, c=\"olive\")\n",
    "    plt.scatter(traj[-1, :n, :, 0], traj[-1, :n, :, 1], s=4, alpha=1, c=\"blue\")\n",
    "    plt.scatter(traj[0, m, :, 0], traj[0, m, :, 1], s=10, alpha=0.8, c=\"green\")\n",
    "    plt.scatter(traj[:, m, :, 0], traj[:, m, :, 1], s=0.2, alpha=0.2, c=\"orange\")\n",
    "    plt.scatter(traj[-1, m, :, 0], traj[-1, m, :, 1], s=4, alpha=1, c=\"red\")\n",
    "    plt.legend([\"Prior sample z(S)\", \"Flow\", \"z(0)\"])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580d2996",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trajectories1(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa05c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1 = torch.rand_like(torch.ones(10, 5, 3))\n",
    "tensor2 = torch.rand_like(torch.ones(10, 5, 3))\n",
    "print(tensor1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8159bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_set_feats(tensor: torch.Tensor) -> torch.Tensor:\n",
    "#    if len(tensor.shape) != 3:\n",
    "#        raise ValueError(\"Input tensor must be of shape (batch, set_size, feats)\")\n",
    "#    mean_set = torch.mean(tensor, dim=-2)\n",
    "#    sum_set = torch.sum(tensor, dim=-2)\n",
    "#    mean_feat = torch.mean(tensor, dim=-1)\n",
    "#    mean_set_feat = torch.mean(mean_feat, dim=-1).unsqueeze(-1)\n",
    "#    sum_set_feat = torch.sum(mean_feat, dim=-1).unsqueeze(-1)\n",
    "#    cat = torch.cat([mean_set, sum_set, mean_set_feat, sum_set_feat], dim=-1)\n",
    "#    return cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700ea790",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"function: {calc_set_feats(tensor1).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebf553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_set = torch.mean(tensor1, dim=-2)\n",
    "sum_set = torch.sum(tensor1, dim=-2)\n",
    "print(mean_set.shape)\n",
    "print(sum_set.shape)\n",
    "print(f\"tensor1: {tensor1.shape}\")\n",
    "mean_feat = torch.mean(tensor1, dim=-1)\n",
    "print(mean_feat.shape)\n",
    "mean_set_feat = torch.mean(mean_feat, dim=-1).unsqueeze(-1)\n",
    "sum_set_feat = torch.sum(mean_feat, dim=-1).unsqueeze(-1)\n",
    "print(mean_set_feat.shape)\n",
    "print(sum_set_feat.shape)\n",
    "cat = torch.cat([mean_set, sum_set, mean_set_feat, sum_set_feat], dim=-1)\n",
    "print(cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72553068",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = tensor1 - tensor2\n",
    "print(diff.shape)\n",
    "sqr = diff**2\n",
    "print(sqr.shape)\n",
    "mn = torch.mean(sqr)\n",
    "print(mn.shape)\n",
    "# loss = torch.mean((vt - ut) ** 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17045d0d",
   "metadata": {},
   "source": [
    "### Optimal Transport Conditional Flow Matching\n",
    "\n",
    "Next we implement optimal transport conditional flow matching. As in the paper, here we have\n",
    "$$\n",
    "\\begin{align}\n",
    "z &= (x_0, x_1) \\\\\n",
    "q(z) &= \\pi(x_0, x_1) \\\\\n",
    "p_t(x | z) &= \\mathcal{N}(x | t * x_1 + (1 - t) * x_0, \\sigma^2) \\\\\n",
    "u_t(x | z) &= x_1 - x_0\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\pi$ is the joint of an exact optimal transport matrix. We first sample random $x_0, x_1$, then resample according to the optimal transport matrix as computed with the python optimal transport package. We use the 2-Wasserstein distance with an $L^2$ ground distance for equivalence with dynamic optimal transport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d5c20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sigma = 0.1\n",
    "dim = 2\n",
    "batch_size = 256\n",
    "model = MLP(dim=dim, time_varying=True)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "start = time.time()\n",
    "for k in range(20000):\n",
    "    optimizer.zero_grad()\n",
    "    t = torch.rand(batch_size, 1)\n",
    "    x0 = sample_8gaussians(batch_size)\n",
    "    x1 = sample_moons(batch_size)\n",
    "    print(x0.shape, x1.shape)\n",
    "    # Resample x0, x1 according to transport matrix\n",
    "    a, b = pot.unif(x0.size()[0]), pot.unif(x1.size()[0])\n",
    "    # return uniform distribution (256 Werte alle mit dem gleichen Wert 1/256)\n",
    "    # print(f\"x0: {x0.size()[0]}\")\n",
    "    # print(a,b)\n",
    "\n",
    "    M = torch.cdist(x0, x1) ** 2\n",
    "    print(f\"M shape: {M.shape}\")\n",
    "    print(f\"M: {M}\")\n",
    "    M = M / M.max()\n",
    "\n",
    "    pi = pot.emd(a, b, M.detach().cpu().numpy())\n",
    "    print(f\"pi shape: {pi.shape}\")\n",
    "    print(f\"pi: {pi[0]}\")\n",
    "\n",
    "    # Sample random interpolations on pi\n",
    "    p = pi.flatten()\n",
    "    p = p / p.sum()\n",
    "    print(f\"P: {p}\")\n",
    "    choices = np.random.choice(pi.shape[0] * pi.shape[1], p=p, size=batch_size)\n",
    "    print(f\"choices.shape: {choices.shape}\")\n",
    "    print(f\"choices: {choices}\")\n",
    "    i, j = np.divmod(choices, pi.shape[1])\n",
    "    print(f\"i: {i}\")\n",
    "    print(f\"j: {j}\")\n",
    "    x0 = x0[i]\n",
    "    x1 = x1[j]\n",
    "    # calculate regression loss\n",
    "    mu_t = x0 * (1 - t) + x1 * t\n",
    "    sigma_t = sigma\n",
    "    x = mu_t + sigma_t * torch.randn(batch_size, dim)\n",
    "    ut = x1 - x0\n",
    "    vt = model(torch.cat([x, t], dim=-1))\n",
    "    loss = torch.mean((vt - ut) ** 2)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (k + 1) % 5000 == 0:\n",
    "        end = time.time()\n",
    "        print(f\"{k+1}: loss {loss.item():0.3f} time {(end - start):0.2f}\")\n",
    "        start = end\n",
    "        node = NeuralODE(\n",
    "            torch_wrapper(model), solver=\"dopri5\", sensitivity=\"adjoint\", atol=1e-4, rtol=1e-4\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            traj = node.trajectory(\n",
    "                sample_8gaussians(1024),\n",
    "                t_span=torch.linspace(0, 1, 100),\n",
    "            )\n",
    "            plot_trajectories(traj)\n",
    "torch.save(model, f\"{savedir}/otcfm_v1.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ceddc4dc",
   "metadata": {},
   "source": [
    "# Other tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546cefdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.distributions import Normal\n",
    "from tqdm import tqdm\n",
    "from zuko.utils import odeint\n",
    "\n",
    "from src.models.components.diffusion import VPDiffusionSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f03276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        hidden_features: List[int] = [64, 64],\n",
    "    ):\n",
    "        layers = []\n",
    "\n",
    "        for a, b in zip(\n",
    "            [in_features] + hidden_features,\n",
    "            hidden_features + [out_features],\n",
    "        ):\n",
    "            layers.extend([nn.Linear(a, b), nn.ELU()])\n",
    "\n",
    "        super().__init__(*layers[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c03c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ode_wrapper(torch.nn.Module):\n",
    "    \"\"\"Wraps model to ode solver compatible format.\"\"\"\n",
    "\n",
    "    def __init__(self, model, mask=None, cond=None, diff_config={\"max_sr\": 1, \"min_sr\": 1e-8}):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.diff_sched = VPDiffusionSchedule(**diff_config)\n",
    "        self.cond = cond\n",
    "        self.mask = mask\n",
    "        self.init = True\n",
    "        self.steps = 50\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        # num_samples = x.shape[0]\n",
    "        expanded_shape = [-1] + [1] * (x.dim() - 1)\n",
    "        if self.init:\n",
    "            self.init = False\n",
    "            signal_rates, noise_rates = self.diff_sched(t.view(expanded_shape))\n",
    "            x = x * (signal_rates + noise_rates)\n",
    "        _, noise_rates = self.diff_sched(t.view(expanded_shape))\n",
    "        betas = self.diff_sched.get_betas(t.view(expanded_shape))\n",
    "        return (\n",
    "            0.5 * betas * (x - self.model(t, x) / noise_rates) * 1 / self.steps\n",
    "        )  # TODO add cond and mask for full version\n",
    "        # if self.init:\n",
    "        #    self.init = False\n",
    "        #    t_init = torch.ones(num_samples)\n",
    "        #    signal_rates_init, noise_rates_init = self.diff_sched(t_init.view(expanded_shape))\n",
    "        #    x_init = x * (signal_rates_init + noise_rates_init)\n",
    "        #    betas = self.diff_sched.get_betas(t.view(expanded_shape))\n",
    "        #    return 0.5 * betas * (x_init - self.model(t_init, x_init)/ noise_rates_init) # TODO add cond and mask for full version\n",
    "        # else:\n",
    "        #    _, noise_rates = self.diff_sched(t.view(expanded_shape))\n",
    "        #    betas = self.diff_sched.get_betas(t.view(expanded_shape))\n",
    "        #    return 0.5 * betas * (x - self.model(t, x)/ noise_rates) # TODO add cond and mask for full version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84170c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNF(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        features: int,\n",
    "        frequencies: int = 3,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = MLP(2 * frequencies + features, features, **kwargs)\n",
    "\n",
    "        self.register_buffer(\"frequencies\", 2 ** torch.arange(frequencies) * torch.pi)\n",
    "\n",
    "    def forward(self, t: Tensor, x: Tensor) -> Tensor:\n",
    "        t = self.frequencies * t[..., None]\n",
    "        t = torch.cat((t.cos(), t.sin()), dim=-1)\n",
    "        t = t.expand(*x.shape[:-1], -1)\n",
    "\n",
    "        return self.net(torch.cat((t, x), dim=-1))\n",
    "\n",
    "    def encode(self, x: Tensor) -> Tensor:\n",
    "        return odeint(self, x, 1.0, 0.0, phi=self.parameters())\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        # return odeint(ode_wrapper(self), z, 1.0, 0.0, phi=self.parameters())\n",
    "        node = NeuralODE(self, solver=\"midpoint\", sensitivity=\"adjoint\")  # no wrapper\n",
    "        t_span = torch.linspace(1.0, 0.0, 50)\n",
    "        traj = node.trajectory(z, t_span)\n",
    "        return traj[-1]\n",
    "\n",
    "    def log_prob(self, x: Tensor) -> Tensor:\n",
    "        i = torch.eye(x.shape[-1]).to(x)\n",
    "        i = i.expand(x.shape + x.shape[-1:]).movedim(-1, 0)\n",
    "\n",
    "        def augmented(t: Tensor, x: Tensor, ladj: Tensor) -> Tensor:\n",
    "            with torch.enable_grad():\n",
    "                x = x.requires_grad_()\n",
    "                dx = self(t, x)\n",
    "\n",
    "            jacobian = torch.autograd.grad(dx, x, i, is_grads_batched=True, create_graph=True)[0]\n",
    "            trace = torch.einsum(\"i...i\", jacobian)\n",
    "\n",
    "            return dx, trace * 1e-2\n",
    "\n",
    "        ladj = torch.zeros_like(x[..., 0])\n",
    "        z, ladj = odeint(augmented, (x, ladj), 0.0, 1.0, phi=self.parameters())\n",
    "\n",
    "        return Normal(0.0, z.new_tensor(1.0)).log_prob(z).sum(dim=-1) + ladj * 1e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702dcde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowMatchingLoss2(nn.Module):\n",
    "    def __init__(self, v: nn.Module):\n",
    "        super().__init__()\n",
    "\n",
    "        self.v = v\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        t = torch.rand_like(torch.ones(x.shape[0]))\n",
    "        t = t.unsqueeze(-1).repeat_interleave(x.shape[1], dim=1).unsqueeze(-1)\n",
    "        t = t.type_as(x)\n",
    "        z = torch.randn_like(x)\n",
    "        y = (1 - t) * x + (1e-4 + (1 - 1e-4) * t) * z\n",
    "        u = (1 - 1e-4) * z - x\n",
    "\n",
    "        return (self.v(t.squeeze(-1), y) - u).square().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d743926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowMatchingLoss(nn.Module):\n",
    "    \"\"\"Flow matching loss.\n",
    "\n",
    "    from: https://arxiv.org/abs/2210.02747\n",
    "\n",
    "    Args:\n",
    "        flows (nn.ModuleList): Module list of flows\n",
    "        sigma (float, optional): Sigma. Defaults to 1e-4.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, v: nn.Module, sigma: float = 1e-4):\n",
    "        super().__init__()\n",
    "        self.v = v\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, mask: torch.Tensor = None, cond: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        t = torch.rand_like(torch.ones(x.shape[0]))\n",
    "        t = t.unsqueeze(-1).repeat_interleave(x.shape[1], dim=1).unsqueeze(-1)\n",
    "        t = t.type_as(x)\n",
    "\n",
    "        # logger_loss.debug(f\"t: {t.shape}\")\n",
    "\n",
    "        z = torch.randn_like(x)\n",
    "\n",
    "        # logger_loss.debug(f\"z: {z.shape}\")\n",
    "        y = (1 - t) * x + (self.sigma + (1 - self.sigma) * t) * z\n",
    "\n",
    "        # logger_loss.debug(f\"y: {y.shape}\")\n",
    "        # logger_loss.debug(f\"y grad: {y.requires_grad}\")\n",
    "\n",
    "        u_t = (1 - self.sigma) * z - x\n",
    "        # u_t = u_t * mask\n",
    "\n",
    "        # logger_loss.debug(f\"u_t: {u_t.shape}\")\n",
    "\n",
    "        # temp = y.clone()\n",
    "        # for v in self.flows:\n",
    "        #    temp = v(t.squeeze(-1), temp, mask=mask, cond=cond)\n",
    "        # v_t = temp.clone()\n",
    "        v_t = self.v(t.squeeze(-1), y)\n",
    "\n",
    "        # logger_loss.debug(f\"v_t grad: {v_t.requires_grad}\")\n",
    "        # logger_loss.debug(f\"v_t: {v_t.shape}\")\n",
    "\n",
    "        out = (v_t - u_t).square().mean()\n",
    "        # sqrd = (v_t - u_t).square()\n",
    "        # out = sqrd.sum() / mask.sum()  # mean with ignoring masked values\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35da3bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionLoss(nn.Module):\n",
    "    \"\"\"Diffusion loss.\n",
    "\n",
    "    from https://github.com/rodem-hep/PC-JeDi/blob/main/src/models/pc_jedi.py\n",
    "    Args:\n",
    "        flows (nn.ModuleList): Module list of flows\n",
    "        sigma (float, optional): Sigma. Defaults to 1e-4.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        v: nn.Module,\n",
    "        sigma: float = 1e-4,\n",
    "        loss_name: str = \"huber\",\n",
    "        diff_config={\"max_sr\": 1, \"min_sr\": 1e-8},\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.v = v\n",
    "        self.sigma = sigma\n",
    "        self.mle_loss_weight = False\n",
    "        self.diff_sched = VPDiffusionSchedule(**diff_config)\n",
    "        if loss_name == \"mse\":\n",
    "            self.loss_fn = nn.MSELoss(reduction=\"none\")\n",
    "        elif loss_name == \"huber\":\n",
    "            self.loss_fn = nn.HuberLoss(reduction=\"none\")\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Loss {loss_name} not supported\")\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, mask: torch.Tensor = None, cond: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        # sample random uniform times\n",
    "        t = torch.rand_like(torch.ones(x.shape[0]))\n",
    "        t = t.unsqueeze(-1).repeat_interleave(x.shape[1], dim=1).unsqueeze(-1)\n",
    "        t = t.type_as(x)\n",
    "        # logger_loss.debug(f\"t: {t.shape}\")\n",
    "\n",
    "        # Sample from the gaussian latent space to perturb the point clouds\n",
    "        z = torch.randn_like(x)  # * mask\n",
    "        # logger_loss.debug(f\"z: {z.shape}\")\n",
    "\n",
    "        # renaming for clarity\n",
    "        nodes = x\n",
    "        noises = z\n",
    "        diffusion_times = t.clone()\n",
    "\n",
    "        # logger_loss.debug(f\"times2 {diffusion_times[:,0].shape}\")\n",
    "        # logger_loss.debug(f\"times3 {diffusion_times[:,0].view(-1, 1, 1).shape}\")\n",
    "\n",
    "        # Get the signal and noise rates from the diffusion schedule\n",
    "        signal_rates, noise_rates = self.diff_sched(diffusion_times[:, 0].view(-1, 1, 1))\n",
    "        # print(f\"signal_rates: {signal_rates[:5]}\")\n",
    "        # print(f\"noise_rates: {noise_rates[:5]}\")\n",
    "        # print(f\"signal_rates: {signal_rates[:3]}, noise_rates {noise_rates[:3]}\")\n",
    "\n",
    "        # Mix the signal and noise according to the diffusion equation\n",
    "        noisy_nodes = signal_rates * nodes + noise_rates * noises  # original\n",
    "        # noisy_nodes = noise_rates*nodes + signal_rates*noises # new\n",
    "        # noisy_nodes = (1 - t) * nodes + (self.sigma + (1 - self.sigma) * t) * noises\n",
    "        # logger_loss.debug(f\"noisy_nodes: {noisy_nodes.shape}\")\n",
    "\n",
    "        # Predict the noise using the network\n",
    "        # temp = noisy_nodes.clone()\n",
    "        # for v in self.flows:\n",
    "        pred_noises = self.v(t.squeeze(-1), noisy_nodes)\n",
    "        # pred_noises = temp.clone()\n",
    "        # logger_loss.debug(f\"pred_noises: {pred_noises.shape}\")\n",
    "\n",
    "        # Simple noise loss is for \"perceptual quality\"\n",
    "        # simple_loss = self.loss_fn(noises[mask], pred_noises[mask])\n",
    "        # sqrd = (noises * mask - pred_noises * mask).square()\n",
    "        # simple_loss = sqrd.sum() / mask.sum()  # mean with ignoring masked values\n",
    "        # u_t = (1 - self.sigma) * noises - nodes\n",
    "        u_t = noises\n",
    "        simple_loss = (u_t - pred_noises).square().mean()\n",
    "        # MLE loss is for maximum liklihood training\n",
    "        if self.mle_loss_weight:\n",
    "            betas = self.diff_sched.get_betas(diffusion_times[:, 0].view(-1, 1, 1))\n",
    "            mle_weights = betas / noise_rates\n",
    "            # mle_weights = betas / signal_rates\n",
    "            mle_loss = mle_weights * simple_loss\n",
    "            out = simple_loss.mean() + self.mle_loss_weight * mle_loss.mean()\n",
    "        else:\n",
    "            out = simple_loss.mean()\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccae8293",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = CNF(2, hidden_features=[256] * 3)\n",
    "\n",
    "# Training\n",
    "loss = DiffusionLoss(flow)\n",
    "optimizer = torch.optim.AdamW(flow.parameters(), lr=1e-3)\n",
    "\n",
    "data = sample_moons(4096)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c93fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(4096), ncols=88):\n",
    "    subset = torch.randint(0, len(data), (256,))\n",
    "    x = data[subset]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss(x).backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b563b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(4096, 30, 2)\n",
    "    x = flow.decode(z).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ef2fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b77ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "plt.scatter(x[:10, :, 0], x[:10, :, 1])\n",
    "plt.show()\n",
    "# plt.savefig('moons.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47da3dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def euler_sampler(\n",
    "    model,\n",
    "    diff_sched: VPDiffusionSchedule,\n",
    "    initial_noise: torch.Tensor,\n",
    "    n_steps: int = 50,\n",
    "    keep_all: bool = False,\n",
    "    mask: Optional[torch.Tensor] = None,\n",
    "    ctxt: Optional[torch.BoolTensor] = None,\n",
    "    clip_predictions: Optional[tuple] = None,\n",
    ") -> Tuple[torch.Tensor, list]:\n",
    "    \"\"\"Apply the full reverse process to noise to generate a batch of\n",
    "    samples.\"\"\"\n",
    "\n",
    "    # Get the initial noise for generation and the number of sammples\n",
    "    num_samples = initial_noise.shape[0]\n",
    "\n",
    "    # The shape needed for expanding the time encodings\n",
    "    expanded_shape = [-1] + [1] * (initial_noise.dim() - 1)\n",
    "\n",
    "    # Check the input argument for the n_steps, must be less than what was trained\n",
    "    all_stages = []\n",
    "    delta_t = 1 / n_steps\n",
    "\n",
    "    # The initial variables needed for the loop\n",
    "    t = torch.ones(num_samples)\n",
    "    signal_rates, noise_rates = diff_sched(t.view(expanded_shape))\n",
    "    x_t = initial_noise * (signal_rates + noise_rates)\n",
    "    for step in tqdm(range(n_steps), \"Euler-sampling\", leave=False):\n",
    "        # Take a step using the euler method and the gradient calculated by the ode\n",
    "        x_t += get_ode_gradient(model, diff_sched, x_t, t, mask, ctxt) * delta_t\n",
    "        t -= delta_t\n",
    "\n",
    "        # Keep track of the diffusion evolution\n",
    "        if keep_all:\n",
    "            all_stages.append(x_t)\n",
    "\n",
    "        # Clamp the denoised data for stability\n",
    "        if clip_predictions is not None:\n",
    "            x_t.clamp_(*clip_predictions)\n",
    "\n",
    "    return x_t, all_stages\n",
    "\n",
    "\n",
    "def get_ode_gradient(\n",
    "    model,\n",
    "    diff_sched: VPDiffusionSchedule,\n",
    "    x_t: torch.Tensor,\n",
    "    t: torch.Tensor,\n",
    "    mask: Optional[torch.BoolTensor] = None,\n",
    "    ctxt: Optional[torch.Tensor] = None,\n",
    ") -> torch.Tensor:\n",
    "    expanded_shape = [-1] + [1] * (x_t.dim() - 1)\n",
    "    _, noise_rates = diff_sched(t.view(expanded_shape))\n",
    "    betas = diff_sched.get_betas(t.view(expanded_shape))\n",
    "    # print(f\"t.shape: {t.shape}\")\n",
    "    # print(f\"t.shape: {t.unsqueeze(-1).repeat_interleave(30, dim=-1).shape}\")\n",
    "\n",
    "    return (\n",
    "        0.5\n",
    "        * betas\n",
    "        * (x_t - model(t.unsqueeze(-1).repeat_interleave(30, dim=-1), x_t) / noise_rates)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002a4921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling\n",
    "diff_config = {\"max_sr\": 1, \"min_sr\": 1e-8}\n",
    "diff_sched = VPDiffusionSchedule(**diff_config)\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(4096, 30, 2)\n",
    "    x = euler_sampler(flow, diff_sched, z)[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44b0071",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "plt.scatter(x[:10, :, 0], x[:10, :, 1])\n",
    "plt.show()\n",
    "# plt.savefig('moons.pdf', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pllhome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
